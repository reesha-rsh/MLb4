{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reesha-rsh/MLb4/blob/main/lectures/5.0%20-%20Evaluating%20models%3AValidation%20Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKzHq8xim9UC"
      },
      "source": [
        "# Evaluating Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odkxESF9m9UD"
      },
      "source": [
        "Model evaluation is not just the end point of our machine learning pipeline. Before we handle any data, we want to plan ahead and use techniques and metrics that are suited for our purposes.\n",
        "\n",
        "### <a name=\"1\"></a> 1. Model Evaluation Applications\n",
        "Let's start with a question: **\"Why do we care about performance estimates at all?\"**\n",
        "\n",
        "<a name=\"1.1\"></a>**Generalization performance** - We want to estimate the predictive performance of our model on future (unseen) data.\n",
        "- Ideally, the estimated performance of a model tells how well it performs on unseen data – making predictions on future data is often the main problem we want to solve.\n",
        "\n",
        "<a name=\"1.2\"></a>**Model selection** - We want to increase the predictive performance by tweaking the learning algorithm and selecting the best performing model from a given hypothesis space.\n",
        "- Typically, machine learning involves a lot of experimentation. Running a learning algorithm over a training dataset with different hyperparameter settings and different features will result in different models. Since we are typically interested in selecting the best-performing model from this set, we need to find a way to estimate their respective performances in order to rank them against each other.\n",
        "\n",
        "<a name=\"1.3\"></a>**Algorithm selection** - We want to compare different ML algorithms, selecting the best-performing one.\n",
        "- We are usually not only experimenting with the one single algorithm that we think would be the “best solution” under the given circumstances. More often than not, we want to compare different algorithms to each other, oftentimes in terms of predictive and computational performance.\n",
        "\n",
        "Although these three sub-tasks have all in common that we want to estimate the performance of a model, they all require different approaches.\n",
        "\n",
        "This tutorial will focus on **supervised learning**, a subcategory of machine learning where our target values are known in our available dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66WVWSOxm9UL"
      },
      "source": [
        "### <a name=\"2\"></a>2. Model Evaluation Techniques\n",
        "#### <a name=\"2.1\"></a>Holdout method (simple train/test split)\n",
        "The holdout method is the simplest model evaluation technique. We take our labeled dataset and split it randomly into two parts: A **training set** and a **test set**\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/testing_01.png\" width=\"500\">\n",
        "\n",
        "Then, we fit a model to the training data and predict the labels of the test set.\n",
        "<img src=\"https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/testing_02.png\" width=\"500\">\n",
        "\n",
        "And the fraction of correct predictions constitutes our estimate of the prediction accuracy.\n",
        "<img src=\"https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part1/testing_03.png\" width=\"500\">\n",
        "\n",
        "We really don’t want to train and evaluate our model on the same training dataset, since it would introduce **overfitting**. In other words, we can’t tell whether the model simply memorized the training data or not, or whether it generalizes well to new, unseen data.\n",
        "\n",
        "##### Pros:\n",
        "    + Simple\n",
        "    + Fast\n",
        "\n",
        "##### Cons:\n",
        "    - Not so precise estimate of out-of-sample performance comparing to more advanced techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y2MLlRim9UL"
      },
      "source": [
        "### Be aware.\n",
        "\n",
        "As it was said, you want your validation to mimic your test set as close as possible. And you can make a fair assumprion (that is not always true), that distribution of target on train and not seen data is the same. Then you have to use stratification. Stratification ensures stable distributions across split. That is more than just useful if:\n",
        "\n",
        "    + Dataset is small\n",
        "    + Dataset is unbalanced (target average for binary classification this means average target close to 0 or to 1)\n",
        "    + You have multiclassification task\n",
        "\n",
        "See example below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eUCVrmc05gkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9kFvK3fzm9UM"
      },
      "outputs": [],
      "source": [
        "# import data\n",
        "import pandas as pd\n",
        "\n",
        "titanic_train = pd.read_csv(\"/content/drive/MyDrive/projector_course_data/train_titanic.csv\")\n",
        "\n",
        "# check number of rows & columns\n",
        "titanic_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Hx3hHX3Am9UM"
      },
      "outputs": [],
      "source": [
        "titanic_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaUpDq5Pm9UN"
      },
      "outputs": [],
      "source": [
        "# split dataset to Train and Test parts\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "titanic_features, titanic_label = titanic_train.drop('Survived', axis = 1), titanic_train.Survived\n",
        "titanic_features_train, titanic_features_test, titanic_label_train, titanic_label_test = \\\n",
        "  train_test_split(titanic_features, titanic_label, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csnvl1iGm9UN"
      },
      "outputs": [],
      "source": [
        "titanic_label_train.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNcb04BNm9UN"
      },
      "outputs": [],
      "source": [
        "titanic_label_test.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVE9Gm69m9UN"
      },
      "outputs": [],
      "source": [
        "# fit a model to the training data\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = classifier.fit(titanic_features_train, titanic_label_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJ38vPU8m9UN"
      },
      "outputs": [],
      "source": [
        "# predict the labels of the test set\n",
        "predictions = model.predict(titanic_features_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnt-pKI7m9UN"
      },
      "outputs": [],
      "source": [
        "# compute prediction accuracy\n",
        "from sklearn import metrics\n",
        "titanic_label_test.value_counts(normalize=True)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(titanic_label_test, predictions))\n",
        "print(\"Train Accuracy:\", metrics.accuracy_score(titanic_label_train, model.predict(titanic_features_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvrcYMxWm9UN"
      },
      "outputs": [],
      "source": [
        "titanic_features_train, titanic_features_test, titanic_label_train, titanic_label_test = \\\n",
        "  train_test_split(titanic_features, titanic_label, test_size=0.2, random_state=42, stratify = titanic_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTK-DSS4m9UO"
      },
      "outputs": [],
      "source": [
        "titanic_label_train.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATAw0_QQm9UO"
      },
      "outputs": [],
      "source": [
        "titanic_label_test.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwxm0NYvm9UO"
      },
      "outputs": [],
      "source": [
        "# fit a model to the training data\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "\n",
        "model = classifier.fit(titanic_features_train, titanic_label_train)\n",
        "\n",
        "predictions = model.predict(titanic_features_test)\n",
        "\n",
        "titanic_label_test.value_counts(normalize=True)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(titanic_label_test, predictions))\n",
        "print(\"Train Accuracy:\", metrics.accuracy_score(titanic_label_train, model.predict(titanic_features_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_N3QyVSm9UO"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv(\"/content/drive/MyDrive/projector_course_data/test_titanic.csv\")\n",
        "test.set_index(\"PassengerId\", inplace=True)\n",
        "test[\"Survived\"] = model.predict(test)\n",
        "#test['Survived'].reset_index().to_csv('pred/pred.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "r0B4tQFQm9UO"
      },
      "source": [
        "### <a name=\"2.2\"></a>K-fold Cross-validation\n",
        "K-fold Cross-validation is probably the most common technique for model evaluation and model selection.\n",
        "- We split the dataset into *K* parts and iterate over a dataset set *K* times\n",
        "- In each round one part is used for validation, and the remaining *K-1* parts are merged into a training subset for model evaluation\n",
        "- We compute the cross-validation performance as the arithmetic mean over the *K* performance estimates from the validation sets.\n",
        "<img src=\"https://sebastianraschka.com/images/blog/2016/model-evaluation-selection-part3/kfold.png\" width=\"500\">\n",
        "\n",
        "##### Pros:\n",
        "    + Better estimate of out-of-sample performance than simple train/test split\n",
        "\n",
        "##### Cons:\n",
        "    - Runs \"K\" times slower than simple train/test split\n",
        "\n",
        "If we have **little data** and **enough time**, it's better to always do cross-validation for a more precise estimate of performance.\n",
        "\n",
        "In the following example we will apply k-fold cross validation for Model Selection using *GridSearchCV* function.\n",
        "\n",
        "> #### GridSearchCV main parameters\n",
        ">*sklearn.model_selection.GridSearchCV*\n",
        "\n",
        ">**param_grid**: dict or list of dictionaries.\n",
        "Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings.\n",
        "\n",
        ">**cv**: int, cross-validation generator or an iterable, optional.\n",
        "Determines the cross-validation splitting strategy.\n",
        "\n",
        ">**scoring**: string, callable or None, default=None.\n",
        "Controls what metric to apply to the estimators evaluated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ZlI4HEIHm9UO"
      },
      "source": [
        "### <a name=\"2.2\"></a>LOO or Leave One Out validation\n",
        "LOO validation is a corner case of K-fold cross-validation, where *K* is equal to *N* - number of examples in the dataset.  \n",
        "- We split the dataset into *N* parts, where *i-th* part is the original dataset sans i-th example\n",
        "- In each round i-th example is used for validation, and the remaining *N-1* examples creates a training for model\n",
        "- We compute the cross-validation performance as the arithmetic mean over the same as in K_Fold\n",
        "\n",
        "You can use LOO validation in case you have a small dataset and/or very easy model to train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "85AzeTILm9UO"
      },
      "outputs": [],
      "source": [
        "# fit model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = dict(C=[100, 10, 1, 0.1, 0.01, 0.001, 0.0001])\n",
        "grid_search = GridSearchCV(classifier, param_grid=params, cv=3)\n",
        "\n",
        "%time grid_search.fit(titanic_features_train, titanic_label_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fekp2TVrm9UP"
      },
      "outputs": [],
      "source": [
        "# Best parameters found:\n",
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLq35fgLm9UP"
      },
      "outputs": [],
      "source": [
        "# Average accuracy over K folds for best parameters set\n",
        "print(\"Validation Accuracy\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "GLy7mBU7m9UP"
      },
      "source": [
        "## Splitting dataset into train and validation\n",
        "\n",
        "### Row validation. Random\n",
        "This assumes that rows are independent such as loan default prediction where each row represents a client. This is not always true as if there are family members, you can assume that they also will be able to pay off a loan. Although this dependency can lead to interesting leaks/feature generation depending on whether family members were splitted to different train and test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "CFzR4we-m9UP"
      },
      "source": [
        "Another type of validation construction - is by group. Suppose you have a task to build a model to predict a weather in cities based on previous dates. Then if you know that in test set there are only new unseen cities, you should split yor dataset on train and validation such as there is no records for any city present in both train and validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwPnXVeVm9UP"
      },
      "source": [
        "### Time Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vELIFFdum9UP"
      },
      "source": [
        "Doing **Time validation** in correct way is very important. Suppose you have a task to predict Wikipedia page viewers as in on of previous Kaggle competitions (https://www.kaggle.com/c/web-traffic-time-series-forecasting). What are possible ways to do a validation? Again, it is best to mimic split made by organizers and they split this by date. All before January, 1st, 2017 went to train, all after that date (2 months) - to test. The correct way to perform a split is with **sliding window**(credit for picture to Uber blogpost):\n",
        "\n",
        "\n",
        "<img src=\"http://eng.uber.com/wp-content/uploads/2018/01/image3-4.png\" width=\"500\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1HlAiYa3m9UP"
      },
      "outputs": [],
      "source": [
        "from dateutil.relativedelta import relativedelta\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1AF5QzDOm9UP"
      },
      "outputs": [],
      "source": [
        "sunspots = pd.read_csv(\"/content/drive/MyDrive/projector_course_data/sunspots_2014-2016.csv\", parse_dates=['date'])\n",
        "sunspots.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTpMXzr5m9UP"
      },
      "outputs": [],
      "source": [
        "sunspots.tail(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsnSIqOim9UP"
      },
      "outputs": [],
      "source": [
        "sunspots['Month'] = sunspots[\"date\"].dt.month\n",
        "sunspots['Day'] = sunspots[\"date\"].dt.day\n",
        "sunspots['DayOfWeek'] = sunspots[\"date\"].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2ZBbBnam9UQ"
      },
      "outputs": [],
      "source": [
        "sunspots_train, sunspots_test = sunspots[sunspots[\"date\"] < \"2016-01-01\"], sunspots[sunspots[\"date\"]>=\"2016-01-01\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSG0iLX3m9UQ"
      },
      "outputs": [],
      "source": [
        "def create_validation(df, start_date):\n",
        "    return df.loc[(df[\"date\"] >= pd.to_datetime(start_date) - relativedelta(days=0)) & \\\n",
        "                  (df[\"date\"] <  pd.to_datetime(start_date) + relativedelta(months=6))].index, \\\n",
        "           df.loc[(df[\"date\"] >= pd.to_datetime(start_date) + relativedelta(months=6)) & \\\n",
        "                  (df[\"date\"] <  pd.to_datetime(start_date) + relativedelta(months=12))].index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypDJhnzGm9UQ"
      },
      "outputs": [],
      "source": [
        "train_dates = [\"2014-01-01\", \"2014-07-01\", \"2015-01-01\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nb4ZhS0m9UQ"
      },
      "outputs": [],
      "source": [
        "custom_cv = []\n",
        "for train_date in train_dates:\n",
        "    train_indicies, val_indicies = create_validation(sunspots_train, train_date)\n",
        "    custom_cv.append((train_indicies, val_indicies))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cchCscEWm9UQ"
      },
      "outputs": [],
      "source": [
        "for train_indicies, val_indicies in custom_cv:\n",
        "    print(min(train_indicies), min(val_indicies))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urFIzHARm9UQ"
      },
      "outputs": [],
      "source": [
        "sunspots_features_train = sunspots_train.drop([\"value\", \"date\"], axis = 1)\n",
        "sunspots_label_train = sunspots_train[\"value\"]\n",
        "\n",
        "sunspots_features_test = sunspots_test.drop([\"value\", \"date\"], axis = 1)\n",
        "sunspots_label_test = sunspots_test[\"value\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iaYASuBm9UQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.pipeline import Pipeline\n",
        "regressor = SVR()\n",
        "\n",
        "pipeline_r = Pipeline([(\"regressor\", regressor)])\n",
        "param_grid = [\n",
        "  {\"regressor__C\": [0.01, 0.1, 1, 10, 100, 1000], \"regressor__kernel\": [\"linear\"]},\n",
        "  {\"regressor__C\": [0.01, 0.1, 1, 10, 100, 1000], \"regressor__gamma\": [0.001, 0.0001], \"regressor__kernel\": [\"rbf\"]},\n",
        " ]\n",
        "grid_search = GridSearchCV(pipeline_r, param_grid=param_grid, cv=custom_cv, scoring=metrics.make_scorer(metrics.mean_absolute_error))\n",
        "\n",
        "%time grid_search.fit(sunspots_features_train, sunspots_label_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw6bVrItm9UQ"
      },
      "outputs": [],
      "source": [
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_ljsS7um9UQ"
      },
      "outputs": [],
      "source": [
        "predictions = grid_search.predict(sunspots_features_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUZuP_Vbm9UQ"
      },
      "outputs": [],
      "source": [
        "grid_search.score(sunspots_features_test, sunspots_label_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "prVd-KVZm9UQ"
      },
      "source": [
        "### Group Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "9bFwPm0im9UQ"
      },
      "source": [
        "Group can refer to user id, store, city or any other entity. Another type of validation construction - is by group. Suppose you have a task to build a model to predict a weather in cities based on previous dates. Then if you know that in test set there are only new unseen cities, you should split yor dataset on train and validation such as there is no records for any city present in both train and validation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pandas_profiling\n",
        "!pip install pandas_profiling"
      ],
      "metadata": {
        "id": "VyDFmWaSE9eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUZPFFF6m9UR"
      },
      "outputs": [],
      "source": [
        "import pandas_profiling as pp\n",
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/projector_course_data/train_titanic.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/projector_course_data/test_titanic.csv\")\n",
        "pp.ProfileReport(train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "P-tjl7O4m9UR"
      },
      "outputs": [],
      "source": [
        "pp.ProfileReport(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq7QT6f-m9UR"
      },
      "source": [
        "## Metrics overview\n",
        "\n",
        "Every competition can rely on different metrics that usually is dictated from business needs. It is important to understand the competition metric and optimize only this metric and not any other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNZjkOfhm9UR"
      },
      "source": [
        "### <a name=\"3\"></a>3. Classification metrics overview\n",
        "Classification problems are probably the most common type of ML problem and as such there are many metrics that can be used to evaluate predictions for these problems. We will review some of them.\n",
        "\n",
        "First note, that many of classifiers return soft labels, or scores for each class, such as probability, while others - hard labels i.e class where target belongs. Soft label can transformed to hard labels for example using threshold for binary classification\n",
        "\n",
        "### <a name=\"3.1\"></a>LogLoss\n",
        "\n",
        "For binary classification, works with soft labels\n",
        "\n",
        "$$ LogLoss = {-\\frac{1}{N} \\sum_{i=1}^{N}(y_{i}log(\\hat{y_{i}})+(1-y_i)log(1-\\hat{y_{i}}))}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "5uXMJdcfm9UR"
      },
      "source": [
        "### <a name=\"3.1\"></a>Accuracy\n",
        "Accuracy simply measures *what percent of your predictions were correct*. It's the ratio between the number of correct predictions and the total number of predictions. The downside is that it is hard to optimize and it cares about hard labels\n",
        "\n",
        "$$accuracy = {\\frac{\\#\\ correct}{\\#\\ predictions}}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH9-hAC6m9UR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "X, y = train.drop('Survived', axis = 1), train.Survived\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42, stratify = y)\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "pipeline = Pipeline([('classifier', classifier)])\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xWGDoM1Km9UR"
      },
      "outputs": [],
      "source": [
        "# calculate accuracy\n",
        "from sklearn import metrics\n",
        "print(metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq5EJFIEm9UR"
      },
      "source": [
        "Accuracy is also the most misused metric. It is really **only suitable** when there are an *equal number of observations in each class* (which is rarely the case) and that all *predictions and prediction errors are equally important*, which is often not the case.\n",
        "\n",
        "### <a name=\"3.2\"></a>Confusion Matrix\n",
        "The confusion matrix is a handy presentation of the accuracy of a model with 2 or more classes. The table **presents predictions** on the x-axis and **accuracy outcomes** on the y-axis. The cells of the table are the number of predictions made by a machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UGhNTcChm9UR"
      },
      "outputs": [],
      "source": [
        "# first argument is true values, second argument is predicted values\n",
        "# this produces a 2x2 numpy array (matrix)\n",
        "conf = metrics.confusion_matrix(y_test, y_pred)\n",
        "print(conf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8bvMPdmDm9UR"
      },
      "source": [
        "|                | Predicted Positive | Predicted  Negative|\n",
        "|:--------------:|--------------------|--------------------|\n",
        "| **Positive Cases** |      TP: 95      |      FN: 15      |\n",
        "| **Negative Cases** |      FP: 20      |      TN: 49     |\n",
        "\n",
        "- **True Positives (TP)**:\n",
        "We correctly predicted that a person will survive: **95**\n",
        "- **True Negatives (TN)**:\n",
        "We correctly predicted that a person will not survive: **49**\n",
        "- **False Positives (FP)**:\n",
        "We incorrectly predicted that a person will survive: **20**\n",
        "- **False Negatives (FN)**:\n",
        "We incorrectly predicted that a person will not survive: **15**\n",
        "\n",
        "\n",
        "\n",
        "Confusion matrix allows you to compute various classification metrics, and these metrics can guide your model selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LpRSFLKmm9UR"
      },
      "outputs": [],
      "source": [
        "# slice confusion matrix into four pieces for future use\n",
        "TP = conf[1, 1]\n",
        "TN = conf[0, 0]\n",
        "FP = conf[0, 1]\n",
        "FN = conf[1, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "sw6fBFTym9UR"
      },
      "source": [
        "You can learn more about the [Confusion Matrix on the Wikipedia article](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
        "\n",
        "\n",
        "### <a name=\"3.3\"></a>Precision & Recall\n",
        "Precision and recall are actually two metrics. But they are often used together.\n",
        "\n",
        "**Precision** answers the question: *What percent of positive predictions were correct?*\n",
        "\n",
        "$$precision = {\\frac{\\#\\ true\\ positive}{\\#\\ true\\ positive + \\#\\ false\\ positive}}$$\n",
        "\n",
        "**Recall** answers the question: *What percent of the positive cases did you catch?*\n",
        "\n",
        "\n",
        "$$recall = {\\frac{\\#\\ true\\ positive}{\\#\\ true\\ positive + \\#\\ false\\ negative}}$$\n",
        "\n",
        "![](http://www.kdnuggets.com/images/precision-recall-relevant-selected.jpg)\n",
        "\n",
        "See also a very good explanation of [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) in Wikipedia.\n",
        "\n",
        "[To the table of contents](#0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gkNUtC4Im9UR"
      },
      "outputs": [],
      "source": [
        "# calculate precision\n",
        "precision = TP / float(TP + FP)\n",
        "\n",
        "assert precision == metrics.precision_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dacc0crVm9US"
      },
      "outputs": [],
      "source": [
        "# calculate recall\n",
        "recall = TP / float(TP + FN)\n",
        "\n",
        "assert recall == metrics.recall_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYp_dfdGm9US"
      },
      "source": [
        "### <a name=\"3.4\"></a>F1-score\n",
        "The F1-score (sometimes known as the balanced F-beta score) is a single metric that combines both precision and recall via their harmonic mean:\n",
        "\n",
        "$$F_1 = 2 {\\frac{precision * recall}{precision + recall}}$$\n",
        "\n",
        "Unlike the arithmetic mean, the harmonic mean tends toward the smaller of the two elements. Hence the F1 score will be small if either precision or recall is small.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVIVlk9Mm9US"
      },
      "outputs": [],
      "source": [
        "# calculate f1-score\n",
        "f1 = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "assert f1 == metrics.f1_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpdKh-mEm9US"
      },
      "source": [
        "### <a name=\"3.4\"></a>ROC AUC\n",
        "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n",
        "\n",
        "More details here:\n",
        "http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
        "\n",
        "![](https://i.stack.imgur.com/5x3Xj.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TaG7GbS4m9US"
      },
      "source": [
        "### <a name=\"3.5\"></a>Classification Report\n",
        "Scikit-learn does provide a convenience report when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures.\n",
        "\n",
        "The **classification_report()** function displays the precision, recall, f1-score and support for each class. (*support* is the number of occurrences of each class in *y_true*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bpSQMYMVm9US"
      },
      "outputs": [],
      "source": [
        "# print a report on the binary classification problem\n",
        "print(metrics.classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wrYA3aVdm9US"
      },
      "source": [
        "### <a name=\"4\"></a>4. Regression metrics overview\n",
        "\n",
        "\n",
        "### <a name=\"3.1\"></a>MSE (L2 Loss) and RMSE\n",
        "MSE measures your mean square error from target:\n",
        "\n",
        "\n",
        "$$ MSE = {\\frac{1}{N} \\sum_{i=1}^{N}(y_{i}-\\hat{y_{i}})^2}$$\n",
        "\n",
        "$$ RMSE = {\\sqrt{MSE}}$$\n",
        "\n",
        "RMSE and MSE is similiar in terms of minimizers - value minimizes RMSE **if and only if** it minimizes MSE. This means that in terms of competitions we can optimize MSE instead of RMSE. In fact it is easier to work with MSE. But there is a little bit of difference between the two for gradient-based models. The gradient of RMSE with respect to i-th prediction is basically equal to gradient of MSE multiplied by some value. The value doesn't depend on the index I. It means that travelling along MSE gradient is equivalent to traveling along RMSE gradient but with a different flowing rate and the flowing rate depends on MSE score itself. So, it is kind of dynamic.So even though RMSE and MSE are really similar in terms of models scoring, they can be not immediately interchangeable for gradient based methods. We will probably need to adjust some parameters like the learning rate.\n",
        "\n",
        "To see model performance in terms of baseline mean usually R-squared is used. Or Adjusted R-squared to penalize for model parameters/features\n",
        "\n",
        "$$ R^2 = {\\frac{MSE}{\\frac{1}{N} \\sum_{i=1}^{N}(y_{i}-\\bar{y_{i}})^2}}$$\n",
        "\n",
        "R_squared is between 0 and 1.\n",
        "\n",
        "In finance usually is used MAE metric\n",
        "\n",
        "$$ MAE = {\\frac{1}{N} \\sum_{i=1}^{N}|y_{i}-\\hat{y_{i}}|} $$\n",
        "\n",
        "It is not differentiable in 0, but one can simply overcome that by coding simple *if else* condition. LightGBM can use MAE while xgboost **cannot**\n",
        "\n",
        "If you care more about **relative** error  - **MSPE** or **MAPE** can be used. They are quite similar to MSE and MAE but incorporate error to relative values rather than absolute\n",
        "\n",
        "If you care more about error for different values, you can apply a function to prediction and target before going into MSE. For example taking a **log(y+1)** will introduce (R)MSLE metric that penalizes more for mistakes for smaller number and less for larger"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <a name=\"4\"></a>5. What to do with all these metrics?\n",
        "\n",
        "#### OPTIMIZE\n",
        "\n",
        "In fact there is often the case that model optimizes different metric from what you want it to optimize. Your possible actions are:\n",
        " + Find the model that optimizes your metric. LogLoss, MSE are present in most libraries\n",
        " + Create your own loss function and pass it to the model such as xgboost. You need to write your own derivatives\n",
        " + Preprocess your original target, for example use log(y+1) and RMSE instead of RMSLE\n",
        " + Postprocess your output predictions if you need accuracy\n",
        " + Run desired model with early stopping. This means optimize default loss, monitor your metric and stop training if you see your metric is not improving"
      ],
      "metadata": {
        "collapsed": false,
        "id": "f1u442L8m9US"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final thoughts\n",
        "\n",
        "* Try to reproduce the train/validation split (distribution) as you have in a test or on real-life data\n",
        "* If there is a huge gap between test and real-life results, while train and val scores are similar - you have a leak in data. Carefully review your EDA. Try to remove most predictive features and compare the results\n",
        "* Always try to look not only at F1 score but also at precision and recall (FP, FN) to find out when your model is wrong\n",
        "* Holdout validation works very well when you have a lot of data points in Neural Networks for example\n",
        "* Don't waste your time on building complex models to see if your validation is working. You should see this even submitting a constant value. Concentrate on very simple models such as linear/logistic regression for this\n",
        "* If you have time and ran out of ideas, you can use the next trick - concatenate train and test sets. Create a variable that will have a value 'train' for examples in the train set or 'test' otherwise. Build a classifier that will try to predict whether an example belongs to either of two groups. * After that select the top examples with the highest probability to be included in the test and make them your validation.\n",
        "* There is a possibility to GridSearch and compare algorithms using the statistical significance of Student criteria. The link to review this idea https://youtu.be/HT3QpRp2ewA?t=1071"
      ],
      "metadata": {
        "collapsed": false,
        "id": "pwWF3aUEm9US"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3jDzks1I5bDn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "projector_course",
      "language": "python",
      "name": "projector_course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "r0B4tQFQm9UO",
        "jNZjkOfhm9UR",
        "cpdKh-mEm9US",
        "wrYA3aVdm9US",
        "f1u442L8m9US"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}