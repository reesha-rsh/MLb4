{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bagging homework.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reesha-rsh/MLb4/blob/main/Homework/HW8.%20Bagging_Random%20Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Use any binary classification dataset\n",
        "2. Define validation strategy and use it for all next steps without changes\n",
        "3. Train decision tree model and estimate performance on validation"
      ],
      "metadata": {
        "id": "prVttCvhr4j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Validation approach: We have a small amount of data so I will use a **K Fold** method\n",
        "*   Metric: I plan to optimize **fbeta score** with beta 0.5 to give more weight for precision, thus minimizing false positives - reducing the prediction that a passenger survived when he actually did not.\n",
        "\n"
      ],
      "metadata": {
        "id": "9-nzJAuFsUk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_full = pd.read_csv(\"/content/drive/MyDrive/MLb4/EDA Titanic/train.csv\")\n",
        "test_full = pd.read_csv(\"/content/drive/MyDrive/MLb4/EDA Titanic/test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J_TyHYFr5mz",
        "outputId": "dacd99f0-f3b9-42aa-e9da-58801bfd1217"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(df,age_median,fare_median):\n",
        "  useless_features = ['Name','Ticket','Cabin']\n",
        "  data_cleaned = df\n",
        "  data_cleaned = data_cleaned.drop(columns = useless_features)\n",
        "\n",
        "  # generate binary values using get_dummies\n",
        "  data_cleaned = pd.get_dummies(data_cleaned, columns=['Sex'],prefix=[\"Sex\"])\n",
        "  data_cleaned = pd.get_dummies(data_cleaned, columns=['Embarked'],prefix=[\"Embarked\"])\n",
        "\n",
        "  # Check for NaN values in the DataFrame\n",
        "  nan_mask = data_cleaned.isnull()\n",
        "  # Count the number of NaN values in each column\n",
        "  nan_count_per_column = data_cleaned.isnull().sum()\n",
        "\n",
        "  data_cleaned['Age'] = data_cleaned['Age'].fillna(age_median)\n",
        "  data_cleaned['Fare'] = data_cleaned['Fare'].fillna(fare_median)\n",
        "\n",
        "  return data_cleaned\n"
      ],
      "metadata": {
        "id": "0ffTCrWwsHXY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_columns = ['Pclass',\t'Age',\t'SibSp',\t'Parch',\t'Fare',\t'Sex_female',\t'Sex_male',\t'Embarked_C',\t'Embarked_Q',\t'Embarked_S']"
      ],
      "metadata": {
        "id": "5kw_X7rzsJSW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get medians that will fill NaNs in generate func\n",
        "age_median = train_full['Age'].median()\n",
        "fare_median = train_full['Fare'].median()"
      ],
      "metadata": {
        "id": "OWmmUDuOsKx5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = generate(train_full,age_median=age_median,fare_median=fare_median)\n",
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UA9XIOaqsL9u",
        "outputId": "39565f73-365d-4b73-b7a5-559a535fcf73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  Sex_female  \\\n",
              "0              1         0       3  22.0      1      0   7.2500           0   \n",
              "1              2         1       1  38.0      1      0  71.2833           1   \n",
              "2              3         1       3  26.0      0      0   7.9250           1   \n",
              "3              4         1       1  35.0      1      0  53.1000           1   \n",
              "4              5         0       3  35.0      0      0   8.0500           0   \n",
              "..           ...       ...     ...   ...    ...    ...      ...         ...   \n",
              "886          887         0       2  27.0      0      0  13.0000           0   \n",
              "887          888         1       1  19.0      0      0  30.0000           1   \n",
              "888          889         0       3  28.0      1      2  23.4500           1   \n",
              "889          890         1       1  26.0      0      0  30.0000           0   \n",
              "890          891         0       3  32.0      0      0   7.7500           0   \n",
              "\n",
              "     Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
              "0           1           0           0           1  \n",
              "1           0           1           0           0  \n",
              "2           0           0           0           1  \n",
              "3           0           0           0           1  \n",
              "4           1           0           0           1  \n",
              "..        ...         ...         ...         ...  \n",
              "886         1           0           0           1  \n",
              "887         0           0           0           1  \n",
              "888         0           0           0           1  \n",
              "889         1           1           0           0  \n",
              "890         1           0           1           0  \n",
              "\n",
              "[891 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-fb29e357-0a14-4476-9f68-ab8e97fdf1d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>887</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>888</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>889</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>890</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>891</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows Ã— 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb29e357-0a14-4476-9f68-ab8e97fdf1d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-cf8667a2-1052-4588-bb0c-d47889cd4459\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cf8667a2-1052-4588-bb0c-d47889cd4459')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-cf8667a2-1052-4588-bb0c-d47889cd4459 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fb29e357-0a14-4476-9f68-ab8e97fdf1d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fb29e357-0a14-4476-9f68-ab8e97fdf1d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 42"
      ],
      "metadata": {
        "id": "sex7kbQwsTi-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "AFK96rfx1qDb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_validation, y_train, y_validation = train_test_split(train[features_columns], train['Survived'], test_size=0.2, random_state=random_state, stratify=train['Survived'])\n"
      ],
      "metadata": {
        "id": "ZInb3Rpw2G_0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = train[features_columns]\n",
        "# y_train = train['Survived']"
      ],
      "metadata": {
        "id": "yp_2dCMMuqe-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import make_scorer, fbeta_score, classification_report, accuracy_score\n",
        "from sklearn import metrics\n"
      ],
      "metadata": {
        "id": "5ln5QTEhseo2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)"
      ],
      "metadata": {
        "id": "ISAPU70CscfS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom scoring function with the desired beta value\n",
        "beta = 0.5\n",
        "custom_scorer = make_scorer(fbeta_score, beta=beta)"
      ],
      "metadata": {
        "id": "OssvR8VFsaCL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import make_scorer, fbeta_score\n",
        "\n",
        "\n",
        "\n",
        "# Define the DecisionTreeClassifier model\n",
        "classifier = DecisionTreeClassifier(random_state=random_state)\n",
        "\n",
        "# Define the hyperparameter grid to search over\n",
        "param_grid = {\n",
        "    'criterion': ['gini'],\n",
        "    'splitter': ['random'],\n",
        "    'max_depth': [None, 3, 5, 7, 9, 11],\n",
        "    'min_samples_leaf': [1,  3,  5,  7,  9,  11],\n",
        "    'max_features': [ 1,  3,  5,  7,  9,  10],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize the GridSearchCV object with the DecisionTreeClassifier, hyperparameter grid, and custom scorer\n",
        "grid_search = GridSearchCV(classifier, param_grid, scoring=custom_scorer, cv=skf)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the corresponding model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Optionally, you can evaluate the model on the full data using the F-beta score with beta=0.1\n",
        "y_pred = best_model.predict(X_validation)\n",
        "\n",
        "fbeta = fbeta_score(y_validation, y_pred, beta=beta)\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(metrics.classification_report(y_validation, y_pred))\n",
        "print(metrics.confusion_matrix(y_validation, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhjMorzGsjBd",
        "outputId": "4de6adbb-1221-43a2-873c-c9f6ff2ef7ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 5, 'max_features': 3, 'min_samples_leaf': 9, 'splitter': 'random'}\n",
            "F-beta Score (beta=0.5): 0.7736\n",
            "Accuracy: 0.7989\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85       110\n",
            "           1       0.84      0.59      0.69        69\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.81      0.76      0.77       179\n",
            "weighted avg       0.80      0.80      0.79       179\n",
            "\n",
            "[[102   8]\n",
            " [ 28  41]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train bagging model with decision tree as a base model and estimate performance on validation"
      ],
      "metadata": {
        "id": "zlt_4XlqviTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n"
      ],
      "metadata": {
        "id": "UcyBLQj0wd2L"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"max_features\": [1,  3,  5,  7,  9,  10],\n",
        "    \"max_samples\": [0.7, 0.8, 0.9],\n",
        "    \"estimator__max_depth\": [None, 3, 5, 7, 9, 11],\n",
        "    \"estimator__min_samples_leaf\": [1,  3,  5,  7,  9,  11],\n",
        "    'estimator__criterion': ['gini'],\n",
        "    'estimator__splitter': ['random'],\n",
        "    'estimator__class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=random_state)\n",
        "bg = BaggingClassifier(dt, random_state=random_state, n_estimators=25)\n",
        "\n",
        "bag_grid_search = GridSearchCV(bg, parameters, scoring=custom_scorer, cv=skf)\n",
        "\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "bag_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the corresponding model\n",
        "best_params = bag_grid_search.best_params_\n",
        "best_model = bag_grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Optionally, you can evaluate the model on the full data using the F-beta score with beta=0.1\n",
        "y_pred = best_model.predict(X_validation)\n",
        "\n",
        "fbeta = fbeta_score(y_validation, y_pred, beta=beta)\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(metrics.classification_report(y_validation, y_pred))\n",
        "print(metrics.confusion_matrix(y_validation, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok_Zjsn-vjmR",
        "outputId": "54ee358c-818f-4a54-8b03-2149d14cc0e2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'estimator__class_weight': 'balanced', 'estimator__criterion': 'gini', 'estimator__max_depth': 7, 'estimator__min_samples_leaf': 1, 'estimator__splitter': 'random', 'max_features': 9, 'max_samples': 0.8}\n",
            "F-beta Score (beta=0.5): 0.7348\n",
            "Accuracy: 0.7877\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       110\n",
            "           1       0.75      0.67      0.71        69\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.78      0.77      0.77       179\n",
            "weighted avg       0.79      0.79      0.78       179\n",
            "\n",
            "[[95 15]\n",
            " [23 46]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RANDOM FOREST"
      ],
      "metadata": {
        "id": "bZjVu-6e3SCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "xtIj2iqX41Ma"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"max_features\": [1,  3,  5,  7,  9,  10],\n",
        "    \"max_samples\": [0.7, 0.8, 0.9],\n",
        "    \"max_depth\": [None, 3, 5, 7, 9, 11],\n",
        "    \"min_samples_leaf\": [1,  3,  5,  7,  9,  11],\n",
        "    'criterion': ['gini'],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators=25, random_state=random_state, n_jobs=-1, oob_score=True)\n",
        "forest_grid_search = GridSearchCV(rfc, parameters, scoring=custom_scorer, cv=skf)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "forest_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the corresponding model\n",
        "best_params = forest_grid_search.best_params_\n",
        "best_model = forest_grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Optionally, you can evaluate the model on the full data using the F-beta score with beta=0.1\n",
        "y_pred = best_model.predict(X_validation)\n",
        "\n",
        "fbeta = fbeta_score(y_validation, y_pred, beta=beta)\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(metrics.classification_report(y_validation, y_pred))\n",
        "print(metrics.confusion_matrix(y_validation, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxzd9DVI4IO6",
        "outputId": "362a1f90-381e-44a9-9af8-bd6933279100"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 9, 'max_features': 3, 'max_samples': 0.7, 'min_samples_leaf': 1}\n",
            "F-beta Score (beta=0.5): 0.7475\n",
            "Accuracy: 0.7933\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       110\n",
            "           1       0.78      0.65      0.71        69\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.79      0.77      0.77       179\n",
            "weighted avg       0.79      0.79      0.79       179\n",
            "\n",
            "[[97 13]\n",
            " [24 45]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMms33J4jeDZ"
      },
      "source": [
        "\n",
        "# 5. Write your own bagging implementation:\n",
        "  <br>5.1. Define init for our CustomBaggingClassifier\n",
        "  <br>5.2. Write fit as described in lecture: divide train data on n parts (`n_estimators` in CustomBaggingClassifier), train `base_estimator` on each part and save these models inside class\n",
        "  <br>5.3. For predictions we should use all saved models and combine their predictions (as voting)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "zNO0I89bhHn0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "fcLfBRrXk3r2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import clone"
      ],
      "metadata": {
        "id": "sfSS2RC8IZqJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_trees(tree1, tree2):\n",
        "    if hash(tree1.__dict__.values())==hash(tree2.__dict__.values()):\n",
        "        # the trees have both been trained\n",
        "        if tree1.tree_ != None and tree2.tree_ != None:\n",
        "            try: # the tree values are matching arrays\n",
        "                return np.array_equal(tree1.tree_.value, tree2.tree_.value)\n",
        "            except: # they do not match\n",
        "                return False\n",
        "        elif tree1.tree_ != None or tree2.tree_ != None:\n",
        "            # XOR of the trees is not trained\n",
        "            return False\n",
        "        else: # Neither has been trained\n",
        "            return True\n",
        "    else: # the params are different\n",
        "        return False"
      ],
      "metadata": {
        "id": "49Z_AD4goQjL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBaggingClassifier:\n",
        "    def __init__(self, base_estimator, n_estimators, max_samples=1.0, max_features=1.0, random_state=None):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.estimators = []\n",
        "\n",
        "\n",
        "\n",
        "    def bootstrap_sampling(self, X, y):\n",
        "        # Implement bootstrap sampling to randomly select subsets of data\n",
        "        num_samples = X.shape[0]\n",
        "        if isinstance(self.max_samples, int):\n",
        "            sample_size = self.max_samples\n",
        "        elif isinstance(self.max_samples, float):\n",
        "            sample_size = int(self.max_samples * num_samples)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid type for max_samples. It must be int or float.\")\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        if isinstance(self.max_features, int):\n",
        "            max_features = self.max_features\n",
        "        elif isinstance(self.max_features, float):\n",
        "            max_features = max(1, int(self.max_features * n_features))\n",
        "        else:\n",
        "            raise ValueError(\"Invalid type for max_features. It must be int or float.\")\n",
        "\n",
        "        sample_indices = np.random.choice(num_samples, size=sample_size, replace=True)\n",
        "        sample_features = np.random.choice(n_features, size=max_features, replace=True)\n",
        "\n",
        "        X_sampled = X.iloc[sample_indices,sample_features]\n",
        "        y_sampled = y.iloc[sample_indices]\n",
        "        return X_sampled, y_sampled\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Get a random subset of the data using bootstrap sampling\n",
        "            X_sampled, y_sampled = self.bootstrap_sampling(X, y)\n",
        "\n",
        "            # Train a new base estimator and store it in the list\n",
        "            estimator = clone(self.base_estimator)  # Clone without the random_state\n",
        "            estimator.set_params(random_state=self.random_state)  # Set random_state here\n",
        "            estimator.fit(X_sampled, y_sampled)\n",
        "\n",
        "            # Check if estimators are the same\n",
        "            if self.estimators:\n",
        "              if compare_trees(estimator, self.estimators[-1]):\n",
        "                raise ValueError(\"Estimators are the same.\")\n",
        "\n",
        "            self.estimators.append(estimator)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Make predictions using majority voting for classification\n",
        "        predictions = np.zeros((X.shape[0], len(self.estimators)))\n",
        "\n",
        "        for index, estimator in enumerate(self.estimators):\n",
        "\n",
        "            predictions[:, index] = estimator.predict(X.loc[:,estimator.feature_names_in_])\n",
        "\n",
        "        # Take the majority vote to make the final prediction\n",
        "        final_predictions = np.apply_along_axis(lambda x: np.bincount(x.astype('int')).argmax(), axis=1, arr=predictions)\n",
        "\n",
        "        return final_predictions\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'base_estimator': self.base_estimator,\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'max_samples': self.max_samples,\n",
        "            'max_features': self.max_features,\n",
        "            'random_state': self.random_state\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "4dFu1IPnG7T7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"max_features\": [  3 ],\n",
        "    \"max_samples\": [0.7],\n",
        "    \"base_estimator__max_depth\": [None],\n",
        "    \"base_estimator__min_samples_leaf\": [1],\n",
        "    'base_estimator__criterion': ['gini'],\n",
        "    'base_estimator__splitter': ['random'],\n",
        "    'base_estimatorr__class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "cbg = CustomBaggingClassifier(dt, n_estimators=2)\n",
        "\n",
        "cbag_grid_search = GridSearchCV(cbg, parameters, scoring=custom_scorer, cv=skf)\n",
        "\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "cbag_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the corresponding model\n",
        "best_params = cbag_grid_search.best_params_\n",
        "best_model = cbag_grid_search.best_estimator_\n",
        "best_score = cbag_grid_search.best_score_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best score:\", best_params)\n",
        "\n",
        "\n",
        "# Optionally, you can evaluate the model on the full data using the F-beta score with beta=0.1\n",
        "y_pred = best_model.predict(X_validation)\n",
        "\n",
        "fbeta = fbeta_score(y_validation, y_pred, beta=beta)\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(metrics.classification_report(y_validation, y_pred))\n",
        "print(metrics.confusion_matrix(y_validation, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL0jBUWxSa9z",
        "outputId": "9238d907-ed23-45d4-cfb0-dde0856ad9e1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': None, 'base_estimator__min_samples_leaf': 1, 'base_estimator__splitter': 'random', 'base_estimatorr__class_weight': 'balanced', 'max_features': 3, 'max_samples': 0.7}\n",
            "F-beta Score (beta=0.5): 0.3468\n",
            "Accuracy: 0.6034\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.87      0.73       110\n",
            "           1       0.46      0.17      0.25        69\n",
            "\n",
            "    accuracy                           0.60       179\n",
            "   macro avg       0.54      0.52      0.49       179\n",
            "weighted avg       0.56      0.60      0.55       179\n",
            "\n",
            "[[96 14]\n",
            " [57 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cbag_grid_search.best_score_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1zb18ovEJ5h",
        "outputId": "a2537770-ab22-486c-c2c9-47af4d7a6abc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.816495190847203"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"max_features\": [1,  3,  5,  7,  9,  10],\n",
        "    \"max_samples\": [0.7, 0.8, 0.9],\n",
        "    \"base_estimator__max_depth\": [None, 3, 5, 7, 9, 11],\n",
        "    \"base_estimator__min_samples_leaf\": [1,  3,  5,  7,  9,  11],\n",
        "    'base_estimator__criterion': ['gini'],\n",
        "    'base_estimator__splitter': ['random'],\n",
        "    'base_estimatorr__class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=random_state)\n",
        "cbg = CustomBaggingClassifier(dt, random_state=random_state, n_estimators=25)\n",
        "\n",
        "cbag_grid_search = GridSearchCV(cbg, parameters, scoring=custom_scorer, cv=skf)\n",
        "\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "cbag_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the corresponding model\n",
        "best_params = cbag_grid_search.best_params_\n",
        "best_model = cbag_grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Optionally, you can evaluate the model on the full data using the F-beta score with beta=0.1\n",
        "y_pred = best_model.predict(X_validation)\n",
        "\n",
        "fbeta = fbeta_score(y_validation, y_pred, beta=beta)\n",
        "accuracy = accuracy_score(y_validation, y_pred)\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(metrics.classification_report(y_validation, y_pred))\n",
        "print(metrics.confusion_matrix(y_validation, y_pred))\n"
      ],
      "metadata": {
        "id": "jOXXiZT4LrlU",
        "outputId": "146db3ba-ccc3-4269-c6e8-ca21e04b7154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': None, 'base_estimator__min_samples_leaf': 7, 'base_estimator__splitter': 'random', 'base_estimatorr__class_weight': 'balanced', 'max_features': 3, 'max_samples': 0.7}\n",
            "F-beta Score (beta=0.5): 0.7347\n",
            "Accuracy: 0.7709\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83       110\n",
            "           1       0.82      0.52      0.64        69\n",
            "\n",
            "    accuracy                           0.77       179\n",
            "   macro avg       0.79      0.72      0.73       179\n",
            "weighted avg       0.78      0.77      0.76       179\n",
            "\n",
            "[[102   8]\n",
            " [ 33  36]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "1 fits failed out of a total of 3240.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "1 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"<ipython-input-28-222fa6dc8b39>\", line 50, in fit\n",
            "    raise ValueError(\"Estimators are the same.\")\n",
            "ValueError: Estimators are the same.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.35776957 0.53950425 0.59928963 0.77517339 0.77160091 0.74788708\n",
            " 0.76991312 0.76574839 0.7821288  0.79583939 0.78826578 0.78686575\n",
            " 0.78624098 0.77716116 0.78047984 0.76422094 0.76489412 0.77761528\n",
            " 0.49520558 0.51529026 0.5402635  0.76577957 0.76176723 0.76877487\n",
            " 0.76022502 0.78154191 0.78299504 0.79201829 0.76328682 0.7865638\n",
            " 0.77571086 0.77768424 0.7813821  0.77273986 0.77870859 0.78493458\n",
            " 0.50229617 0.45348441 0.54136474 0.76911914 0.7375618  0.76846295\n",
            " 0.7988244  0.79797695 0.78315645 0.76898104 0.78616435 0.77573427\n",
            " 0.79537447 0.77547846 0.78189005 0.78876558 0.77131406 0.78261916\n",
            " 0.5300828  0.52375964 0.48287594 0.81649519 0.76809015 0.76733329\n",
            " 0.78226958 0.81166531 0.77491179 0.78799717 0.78957477 0.78014552\n",
            " 0.77686585 0.77442536 0.77001724 0.77465885 0.79085449 0.77359906\n",
            " 0.54492975 0.34967939 0.5752577  0.7651607  0.75844869 0.78628771\n",
            " 0.77457345 0.77931712 0.7777062  0.78096507 0.77373336 0.76926192\n",
            " 0.77780068 0.78279439 0.78050228 0.77769795 0.77735977 0.76586993\n",
            " 0.57044961 0.50865405 0.61784167 0.77930789 0.76866473 0.76487426\n",
            " 0.7738514  0.78306066 0.79492619 0.78097991 0.78248456 0.76679862\n",
            " 0.76307479 0.76901584 0.77777526 0.77639286 0.79172111 0.78331241\n",
            " 0.47921528 0.50324459 0.49889988 0.75836887 0.73473466 0.75844859\n",
            " 0.78559912 0.75184407 0.78370175 0.76960004 0.78765961 0.78820068\n",
            " 0.76725037 0.77792618 0.77498126 0.78151983 0.77894294 0.78916288\n",
            " 0.52878154 0.49731966 0.65826868 0.76254174 0.77537376 0.77275663\n",
            " 0.76010794 0.75944315 0.76489244 0.79220085 0.76934032 0.78361023\n",
            " 0.7817698  0.78020311 0.76988655 0.77618818 0.77650512 0.76578948\n",
            " 0.53682146 0.58481501 0.59950676 0.78273512 0.76499479 0.77586835\n",
            " 0.77836275 0.78837445 0.78051287 0.7715651  0.80660266 0.77615733\n",
            " 0.78364097 0.79119729 0.77826463 0.77076524 0.78426681 0.77097122\n",
            " 0.54861795 0.50248283 0.49665484 0.77904965 0.75756733 0.76188569\n",
            " 0.77604099 0.78429731 0.78552004 0.78564668 0.78243189 0.77164754\n",
            " 0.77771319 0.77527228 0.77271047 0.76867178 0.78003652 0.77397757\n",
            " 0.58706308 0.55240297 0.51656959 0.74561893 0.75949873 0.76103635\n",
            " 0.76782353 0.79394052 0.77134796 0.76776189 0.78612872 0.78997606\n",
            " 0.78121679 0.78140088 0.77772716 0.77636504 0.80241244 0.77557034\n",
            " 0.56952625 0.57074554 0.53390513 0.7508862  0.73605704 0.77728097\n",
            " 0.76604277 0.78988208 0.77793854 0.77393525 0.78397737 0.78745375\n",
            " 0.79363611 0.78372287 0.7756633  0.79829126 0.78135669 0.79374812\n",
            " 0.31994525 0.51689601 0.49801585 0.76721236 0.7606061  0.75385877\n",
            " 0.79321577 0.77595781 0.7871241  0.77466706 0.76656436 0.79399188\n",
            " 0.79465829 0.77606732 0.75993437 0.77975795 0.78556529 0.77695153\n",
            " 0.56708661 0.50261363 0.48384297 0.73630901 0.74124402 0.74875001\n",
            " 0.77313741 0.78146524 0.79422315 0.7837131  0.78155276 0.78589926\n",
            " 0.77101183 0.80074016 0.79507005 0.7872637  0.76721977 0.7768361\n",
            " 0.49705818 0.50639851 0.49390599 0.76626749 0.7420733  0.75808146\n",
            " 0.79239964 0.7867172  0.79374638 0.78243939 0.78099804 0.78780725\n",
            " 0.79115287 0.77632076 0.77964746 0.7778458  0.7936023  0.78224247\n",
            " 0.460791   0.6169879  0.54409349 0.75956684 0.76161888 0.7576558\n",
            " 0.77111976 0.76204289 0.77584294 0.79787442 0.76999262 0.77463332\n",
            " 0.76877689 0.75871309 0.76994748 0.78796962 0.78158987 0.79221068\n",
            " 0.39887036 0.47796179 0.49992675 0.76333017 0.78618214 0.75616876\n",
            " 0.79977578 0.77490679 0.75906461 0.7931799  0.7948428  0.78645245\n",
            " 0.77931039 0.77586249 0.77753589 0.75344668 0.79195009 0.76916147\n",
            " 0.58023631 0.44597852        nan 0.77186393 0.76243083 0.76387971\n",
            " 0.76477686 0.76794839 0.78779074 0.77587286 0.79358766 0.77303299\n",
            " 0.78539083 0.79175125 0.78223241 0.78075659 0.8000958  0.78297208\n",
            " 0.46289829 0.51264623 0.50441498 0.75739468 0.76813761 0.7673225\n",
            " 0.76576678 0.78491262 0.77473844 0.79543305 0.78854166 0.79670731\n",
            " 0.78773658 0.78589837 0.76742171 0.76390621 0.78853742 0.75710046\n",
            " 0.54281187 0.53948829 0.57383414 0.72957498 0.75374534 0.76211738\n",
            " 0.78731046 0.75985683 0.78902667 0.79303146 0.77433442 0.78146274\n",
            " 0.78189637 0.78168542 0.76915498 0.77451958 0.78372074 0.80083753\n",
            " 0.47218587 0.55888194 0.45522813 0.73747097 0.77442656 0.77419458\n",
            " 0.77434503 0.7732646  0.79825558 0.77473745 0.77198813 0.77347695\n",
            " 0.80936694 0.77712318 0.78642947 0.77895201 0.76137732 0.77159588\n",
            " 0.57222033 0.59852215 0.51932573 0.74993018 0.76522166 0.72785543\n",
            " 0.7449177  0.77776754 0.78567581 0.78262944 0.76049021 0.78246441\n",
            " 0.77613045 0.78447502 0.78030556 0.78386525 0.78559457 0.77450802\n",
            " 0.55850235 0.63083066 0.52558733 0.74773383 0.77644514 0.76288428\n",
            " 0.78079351 0.78629077 0.78002009 0.77784399 0.80063822 0.78651181\n",
            " 0.77966233 0.78484724 0.80098696 0.7919455  0.78602611 0.77617414\n",
            " 0.52839197 0.56727299 0.40258625 0.76809095 0.77875306 0.76496225\n",
            " 0.784207   0.77952736 0.76607724 0.7773061  0.77800221 0.78727598\n",
            " 0.7766485  0.76082376 0.78097849 0.78793578 0.77925721 0.76493975\n",
            " 0.5812485  0.46539895 0.46731555 0.75604935 0.75330374 0.73705728\n",
            " 0.78086797 0.79049292 0.76101643 0.78878433 0.78448906 0.78027797\n",
            " 0.77815261 0.78662275 0.770736   0.77292671 0.76321721 0.77575865\n",
            " 0.47976635 0.55101746 0.55885868 0.78546285 0.7465895  0.77149132\n",
            " 0.77869347 0.77798697 0.75156734 0.78454844 0.78504812 0.77845792\n",
            " 0.77747856 0.78216534 0.77235552 0.77542923 0.78502951 0.78249375\n",
            " 0.61314854 0.36617728 0.56449332 0.79619062 0.75471489 0.76645978\n",
            " 0.76999846 0.74991643 0.78602262 0.76814145 0.77317845 0.78816009\n",
            " 0.77630445 0.77236936 0.76042707 0.78437865 0.78863812 0.7736575\n",
            " 0.44809868 0.43703603 0.51664746 0.75200714 0.78002937 0.78577868\n",
            " 0.77865754 0.78543125 0.784598   0.77963219 0.7754762  0.78791687\n",
            " 0.76874408 0.794501   0.77815792 0.78470749 0.77723656 0.78110469\n",
            " 0.63175207 0.52637681 0.64408421 0.75368948 0.7695386  0.76263761\n",
            " 0.79152939 0.77051581 0.77256581 0.77005917 0.78197389 0.77696751\n",
            " 0.80308003 0.79722125 0.76994374 0.76439935 0.7799755  0.76733852\n",
            " 0.5432471  0.3974805  0.46384784 0.73649038 0.77912261 0.75214239\n",
            " 0.77601784 0.76423074 0.78170475 0.78712137 0.78074371 0.76251972\n",
            " 0.80700832 0.79589681 0.77238034 0.77486652 0.77288794 0.76142388\n",
            " 0.58770185 0.61799776 0.63599662 0.77807353 0.73654873 0.75670801\n",
            " 0.77808126 0.78582408 0.77814952 0.77763287 0.7645579  0.78920002\n",
            " 0.77310725 0.77344192 0.78158141 0.7902321  0.78807955 0.78633137\n",
            " 0.52702725 0.55461499 0.57241825 0.78220955 0.73713527 0.76651363\n",
            " 0.77299467 0.77880978 0.79464214 0.79496827 0.78099687 0.78815324\n",
            " 0.79535742 0.77921949 0.79700046 0.77747509 0.7805447  0.78587243\n",
            " 0.43042944 0.36189958 0.4954738  0.74497477 0.74494435 0.77272859\n",
            " 0.78861801 0.7886186  0.797868   0.79301625 0.78218886 0.77554281\n",
            " 0.78346306 0.77691082 0.7852804  0.76693949 0.7807068  0.78608207\n",
            " 0.59303223 0.61318935 0.62000549 0.75212184 0.76395454 0.78111051\n",
            " 0.79267715 0.77444862 0.78475677 0.80800882 0.76480703 0.76829791\n",
            " 0.78434083 0.77217481 0.78427698 0.7921053  0.78626792 0.7849522\n",
            " 0.58752249 0.55437579 0.48024976 0.75537476 0.73059725 0.77274554\n",
            " 0.79046213 0.77858656 0.77214423 0.77335662 0.79562829 0.78365425\n",
            " 0.7753243  0.78429057 0.7994336  0.78581994 0.77458135 0.77882128\n",
            " 0.53029913 0.54146614 0.4848837  0.7535676  0.75123649 0.75798791\n",
            " 0.80373998 0.77661707 0.7780534  0.799166   0.78028443 0.79728595\n",
            " 0.78816537 0.75629578 0.76976434 0.78924746 0.77495137 0.77713452]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Compare performance of sklearn bagging model with your own implementation"
      ],
      "metadata": {
        "id": "fsNWrra-_van"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CustomBaggingClassifier**\n",
        "\n",
        "Best Hyperparameters: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': None, 'base_estimator__min_samples_leaf': 1, 'base_estimator__splitter': 'random', 'base_estimatorr__class_weight': 'balanced', 'max_features': 5, 'max_samples': 0.9}\n",
        "F-beta Score (beta=0.5): 0.7935\n",
        "Accuracy: 0.8103\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.79      0.94      0.86       549\n",
        "           1       0.86      0.60      0.71       342\n",
        "\n",
        "    accuracy                           0.81       891\n",
        "   macro avg       0.83      0.77      0.78       891\n",
        "weighted avg       0.82      0.81      0.80       891\n",
        "\n",
        "[[516  33]\n",
        " [136 206]]"
      ],
      "metadata": {
        "id": "S35q6cG8T_5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BaggingClassifier\n",
        "\n",
        "Best Hyperparameters: {'estimator__class_weight': 'balanced', 'estimator__criterion': 'gini', 'estimator__max_depth': None, 'estimator__min_samples_leaf': 1, 'estimator__splitter': 'random', 'max_features': 5, 'max_samples': 0.7}\n",
        "F-beta Score (beta=0.5): 0.9102\n",
        "Accuracy: 0.9147\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.91      0.96      0.93       549\n",
        "           1       0.93      0.84      0.88       342\n",
        "\n",
        "    accuracy                           0.91       891\n",
        "   macro avg       0.92      0.90      0.91       891\n",
        "weighted avg       0.92      0.91      0.91       891\n",
        "\n",
        "[[527  22]\n",
        " [ 54 288]]"
      ],
      "metadata": {
        "id": "_90_xHDiUM7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have a really big gap between Custom and sklearn bagging classifiers. I don't understand what I am missing in my custom implementation"
      ],
      "metadata": {
        "id": "xar_E9P6TVT_"
      }
    }
  ]
}