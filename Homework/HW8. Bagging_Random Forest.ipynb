{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bagging homework.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reesha-rsh/MLb4/blob/main/Homework/HW8.%20Bagging_Random%20Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. Use any binary classification dataset\n",
        "2. Define validation strategy and use it for all next steps without changes\n",
        "3. Train decision tree model and estimate performance on validation"
      ],
      "metadata": {
        "id": "prVttCvhr4j_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Validation approach: We have a small amount of data so I will use a **K Fold** method\n",
        "*   Metric: I plan to optimize **fbeta score** with beta 0.5 to give more weight for precision, thus minimizing false positives - reducing the prediction that a passenger survived when he actually did not.\n",
        "\n"
      ],
      "metadata": {
        "id": "9-nzJAuFsUk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_full = pd.read_csv(\"/content/drive/MyDrive/MLb4/EDA Titanic/train.csv\")\n",
        "test_full = pd.read_csv(\"/content/drive/MyDrive/MLb4/EDA Titanic/test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4J_TyHYFr5mz",
        "outputId": "5568a1af-4025-4e9c-f5c2-65f5827f3bbc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(df,age_median,fare_median):\n",
        "  useless_features = ['Name','Ticket','Cabin']\n",
        "  data_cleaned = df\n",
        "  data_cleaned = data_cleaned.drop(columns = useless_features)\n",
        "\n",
        "  # generate binary values using get_dummies\n",
        "  data_cleaned = pd.get_dummies(data_cleaned, columns=['Sex'],prefix=[\"Sex\"])\n",
        "  data_cleaned = pd.get_dummies(data_cleaned, columns=['Embarked'],prefix=[\"Embarked\"])\n",
        "\n",
        "  # Check for NaN values in the DataFrame\n",
        "  nan_mask = data_cleaned.isnull()\n",
        "  # Count the number of NaN values in each column\n",
        "  nan_count_per_column = data_cleaned.isnull().sum()\n",
        "\n",
        "  data_cleaned['Age'] = data_cleaned['Age'].fillna(age_median)\n",
        "  data_cleaned['Fare'] = data_cleaned['Fare'].fillna(fare_median)\n",
        "\n",
        "  return data_cleaned\n"
      ],
      "metadata": {
        "id": "0ffTCrWwsHXY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_columns = ['Pclass',\t'Age',\t'SibSp',\t'Parch',\t'Fare',\t'Sex_female',\t'Sex_male',\t'Embarked_C',\t'Embarked_Q',\t'Embarked_S']"
      ],
      "metadata": {
        "id": "5kw_X7rzsJSW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get medians that will fill NaNs in generate func\n",
        "age_median = train_full['Age'].median()\n",
        "fare_median = train_full['Fare'].median()"
      ],
      "metadata": {
        "id": "OWmmUDuOsKx5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = generate(train_full,age_median=age_median,fare_median=fare_median)\n",
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "UA9XIOaqsL9u",
        "outputId": "7e727337-281e-4ac4-ed10-8da04f7c463a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  Sex_female  \\\n",
              "0              1         0       3  22.0      1      0   7.2500           0   \n",
              "1              2         1       1  38.0      1      0  71.2833           1   \n",
              "2              3         1       3  26.0      0      0   7.9250           1   \n",
              "3              4         1       1  35.0      1      0  53.1000           1   \n",
              "4              5         0       3  35.0      0      0   8.0500           0   \n",
              "..           ...       ...     ...   ...    ...    ...      ...         ...   \n",
              "886          887         0       2  27.0      0      0  13.0000           0   \n",
              "887          888         1       1  19.0      0      0  30.0000           1   \n",
              "888          889         0       3  28.0      1      2  23.4500           1   \n",
              "889          890         1       1  26.0      0      0  30.0000           0   \n",
              "890          891         0       3  32.0      0      0   7.7500           0   \n",
              "\n",
              "     Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
              "0           1           0           0           1  \n",
              "1           0           1           0           0  \n",
              "2           0           0           0           1  \n",
              "3           0           0           0           1  \n",
              "4           1           0           0           1  \n",
              "..        ...         ...         ...         ...  \n",
              "886         1           0           0           1  \n",
              "887         0           0           0           1  \n",
              "888         0           0           0           1  \n",
              "889         1           1           0           0  \n",
              "890         1           0           1           0  \n",
              "\n",
              "[891 rows x 12 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-6ae60032-19a2-45fb-a971-72a94cfddd35\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>887</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>888</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>889</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>28.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>890</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>891</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ae60032-19a2-45fb-a971-72a94cfddd35')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-47b50050-bb02-48ea-97bf-dada62371dd8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-47b50050-bb02-48ea-97bf-dada62371dd8')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-47b50050-bb02-48ea-97bf-dada62371dd8 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6ae60032-19a2-45fb-a971-72a94cfddd35 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6ae60032-19a2-45fb-a971-72a94cfddd35');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_features = train[features_columns]\n",
        "titanic_label = train['Survived']"
      ],
      "metadata": {
        "id": "yp_2dCMMuqe-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import make_scorer, fbeta_score, classification_report, accuracy_score\n",
        "from sklearn import metrics\n"
      ],
      "metadata": {
        "id": "5ln5QTEhseo2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 42"
      ],
      "metadata": {
        "id": "sex7kbQwsTi-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)"
      ],
      "metadata": {
        "id": "ISAPU70CscfS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom scoring function with the desired beta value\n",
        "beta = 0.5\n",
        "custom_scorer = make_scorer(fbeta_score, beta=beta)"
      ],
      "metadata": {
        "id": "OssvR8VFsaCL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import make_scorer, fbeta_score\n",
        "\n",
        "\n",
        "\n",
        "# Define the DecisionTreeClassifier model\n",
        "classifier = DecisionTreeClassifier(random_state=random_state)\n",
        "\n",
        "# Define the hyperparameter grid to search over\n",
        "param_grid = {\n",
        "    'criterion': ['gini'],\n",
        "    'splitter': ['random'],\n",
        "    'max_depth': [None, 3, 5, 7, 9, 11],\n",
        "    'min_samples_leaf': [1,  3,  5,  7,  9,  11],\n",
        "    'max_features': [ 1,  3,  5,  7,  9,  11],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize the GridSearchCV object with the DecisionTreeClassifier, hyperparameter grid, and custom scorer\n",
        "grid_search = GridSearchCV(classifier, param_grid, scoring=custom_scorer, cv=skf)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "grid_search.fit(titanic_features, titanic_label)\n",
        "\n",
        "# Get the best hyperparameters and the corresponding model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Optionally, you can evaluate the model on the full data using the F-beta score with beta=0.1\n",
        "y_pred = best_model.predict(titanic_features)\n",
        "\n",
        "fbeta = fbeta_score(titanic_label, y_pred, beta=beta)\n",
        "accuracy = accuracy_score(titanic_label, y_pred)\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(metrics.classification_report(titanic_label, y_pred))\n",
        "print(metrics.confusion_matrix(titanic_label, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhjMorzGsjBd",
        "outputId": "ec9ea0c3-3798-4b35-8598-63ea75c6d77b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 5, 'max_features': 3, 'min_samples_leaf': 3, 'splitter': 'random'}\n",
            "F-beta Score (beta=0.5): 0.7950\n",
            "Accuracy: 0.8249\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.91      0.87       549\n",
            "           1       0.83      0.69      0.75       342\n",
            "\n",
            "    accuracy                           0.82       891\n",
            "   macro avg       0.83      0.80      0.81       891\n",
            "weighted avg       0.83      0.82      0.82       891\n",
            "\n",
            "[[500  49]\n",
            " [107 235]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train bagging model with decision tree as a base model and estimate performance on validation"
      ],
      "metadata": {
        "id": "zlt_4XlqviTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n"
      ],
      "metadata": {
        "id": "UcyBLQj0wd2L"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"max_features\": [1,  3,  5,  7,  9,  11],\n",
        "    \"max_samples\": [0.7, 0.8, 0.9],\n",
        "    \"estimator__max_depth\": [None, 3, 5, 7, 9, 11],\n",
        "    \"estimator__min_samples_leaf\": [1,  3,  5,  7,  9,  11],\n",
        "    'estimator__criterion': ['gini'],\n",
        "    'estimator__splitter': ['random'],\n",
        "    'estimator__class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=random_state)\n",
        "bg = BaggingClassifier(dt, random_state=random_state, n_estimators=25)\n",
        "\n",
        "bag_grid_search = GridSearchCV(bg, parameters, scoring=custom_scorer, cv=skf)\n",
        "\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "bag_grid_search.fit(titanic_features, titanic_label)\n",
        "\n",
        "# Get the best hyperparameters and the corresponding model\n",
        "best_params = bag_grid_search.best_params_\n",
        "best_model = bag_grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Optionally, you can evaluate the model on the full data using the F-beta score with beta=0.1\n",
        "y_pred = best_model.predict(titanic_features)\n",
        "\n",
        "fbeta = fbeta_score(titanic_label, y_pred, beta=beta)\n",
        "accuracy = accuracy_score(titanic_label, y_pred)\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(metrics.classification_report(titanic_label, y_pred))\n",
        "print(metrics.confusion_matrix(titanic_label, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok_Zjsn-vjmR",
        "outputId": "efe9c40a-d5c8-41d3-c396-ba7f48559c0f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'estimator__class_weight': 'balanced', 'estimator__criterion': 'gini', 'estimator__max_depth': None, 'estimator__min_samples_leaf': 1, 'estimator__splitter': 'random', 'max_features': 5, 'max_samples': 0.7}\n",
            "F-beta Score (beta=0.5): 0.9102\n",
            "Accuracy: 0.9147\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       549\n",
            "           1       0.93      0.84      0.88       342\n",
            "\n",
            "    accuracy                           0.91       891\n",
            "   macro avg       0.92      0.90      0.91       891\n",
            "weighted avg       0.92      0.91      0.91       891\n",
            "\n",
            "[[527  22]\n",
            " [ 54 288]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "540 fits failed out of a total of 3240.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "540 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_bagging.py\", line 337, in fit\n",
            "    return self._fit(X, y, self.max_samples, sample_weight=sample_weight)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_bagging.py\", line 420, in _fit\n",
            "    raise ValueError(\"max_features must be <= n_features\")\n",
            "ValueError: max_features must be <= n_features\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.71430498 0.71679576 0.71633465 0.74304914 0.7471006  0.74588019\n",
            " 0.80662236 0.79410173 0.79356709 0.79875247 0.79651709 0.78770364\n",
            " 0.7785034  0.77726393 0.77358508        nan        nan        nan\n",
            " 0.70604143 0.70797718 0.71143161 0.72676215 0.72721507 0.72784528\n",
            " 0.74179865 0.74094208 0.74793345 0.75507703 0.75795124 0.76243563\n",
            " 0.76423826 0.76369263 0.76314516        nan        nan        nan\n",
            " 0.70716367 0.7120902  0.71152305 0.7289758  0.7289758  0.7289758\n",
            " 0.72843369 0.73503357 0.73610217 0.75558243 0.7591955  0.76506676\n",
            " 0.74316506 0.75201184 0.75432218        nan        nan        nan\n",
            " 0.70730021 0.70888677 0.71043111 0.7289758  0.7289758  0.7289758\n",
            " 0.73173378 0.72682593 0.73099583 0.73737707 0.74446023 0.75213955\n",
            " 0.73422653 0.74881385 0.75126456        nan        nan        nan\n",
            " 0.70918389 0.71080513 0.70868174 0.7289758  0.7289758  0.7289758\n",
            " 0.72644115 0.73114885 0.72644115 0.73378115 0.73179323 0.74095771\n",
            " 0.74077017 0.74860126 0.74964994        nan        nan        nan\n",
            " 0.70910436 0.71559652 0.71547175 0.7289758  0.7289758  0.7289758\n",
            " 0.72832711 0.72832711 0.72832711 0.72761777 0.72796714 0.73706221\n",
            " 0.74144624 0.74323721 0.73706179        nan        nan        nan\n",
            " 0.70591736 0.71198598 0.71017219 0.72784528 0.72595931 0.72595931\n",
            " 0.73148133 0.73148133 0.72784528 0.72924879 0.7371783  0.73463427\n",
            " 0.74469774 0.73564857 0.73745024        nan        nan        nan\n",
            " 0.70872691 0.71198598 0.71017219 0.7289758  0.7289758  0.72784528\n",
            " 0.7289758  0.72784528 0.72784528 0.72492219 0.73039163 0.72912641\n",
            " 0.73342694 0.73786208 0.73317691        nan        nan        nan\n",
            " 0.71069511 0.71017219 0.71174153 0.7289758  0.7289758  0.7289758\n",
            " 0.72721507 0.72721507 0.72721507 0.72671901 0.7294076  0.72721507\n",
            " 0.72768116 0.73011164 0.7341285         nan        nan        nan\n",
            " 0.70822621 0.71108644 0.71228182 0.7289758  0.7289758  0.7289758\n",
            " 0.72721507 0.72721507 0.72721507 0.72532911 0.72532911 0.72721507\n",
            " 0.72280175 0.73649174 0.73287801        nan        nan        nan\n",
            " 0.7099167  0.70979556 0.71172962 0.7289758  0.7289758  0.7289758\n",
            " 0.72721507 0.72721507 0.72721507 0.72769164 0.7265796  0.72532911\n",
            " 0.72430162 0.72752056 0.73773914        nan        nan        nan\n",
            " 0.71279026 0.71871402 0.71546727 0.7289758  0.7289758  0.7289758\n",
            " 0.7289758  0.7289758  0.72721507 0.72585466 0.72472715 0.72532911\n",
            " 0.7229264  0.72358687 0.72584454        nan        nan        nan\n",
            " 0.71735866 0.71298016 0.71856932 0.73323809 0.72765931 0.72749548\n",
            " 0.7456537  0.73820196 0.75378701 0.76956655 0.76988898 0.77576202\n",
            " 0.77438278 0.76635739 0.77565385        nan        nan        nan\n",
            " 0.70854252 0.70969293 0.71186781 0.72851466 0.72607725 0.72607725\n",
            " 0.74811251 0.7419685  0.73408035 0.75666564 0.75125186 0.76671324\n",
            " 0.75943293 0.75916137 0.75944765        nan        nan        nan\n",
            " 0.70948093 0.71246879 0.71394794 0.7289758  0.7289758  0.7289758\n",
            " 0.73606746 0.72734976 0.73270486 0.74223939 0.74043788 0.75691743\n",
            " 0.75123039 0.74588508 0.75057658        nan        nan        nan\n",
            " 0.70658698 0.70899561 0.71392172 0.7289758  0.7289758  0.7289758\n",
            " 0.73056273 0.72915897 0.72674734 0.72930168 0.74145591 0.74522588\n",
            " 0.74951893 0.74952627 0.74638709        nan        nan        nan\n",
            " 0.70616252 0.71549451 0.71171966 0.7289758  0.7289758  0.7289758\n",
            " 0.72721507 0.72721507 0.72832711 0.72384091 0.73394963 0.74657915\n",
            " 0.73929597 0.73546663 0.7371931         nan        nan        nan\n",
            " 0.70965427 0.71211072 0.71540914 0.7289758  0.7289758  0.7289758\n",
            " 0.72721507 0.72721507 0.72721507 0.72617232 0.72758292 0.74011449\n",
            " 0.7432657  0.75028301 0.73824563        nan        nan        nan\n",
            " 0.71205351 0.71488357 0.71775474 0.73373943 0.7370731  0.7394283\n",
            " 0.75341212 0.77240376 0.7590327  0.78278775 0.78260745 0.76792798\n",
            " 0.76240999 0.77359296 0.77067006        nan        nan        nan\n",
            " 0.7097835  0.70822621 0.7129422  0.72676215 0.7289758  0.7278729\n",
            " 0.75127754 0.73851948 0.74383874 0.76268574 0.76504504 0.77458205\n",
            " 0.75552172 0.75829721 0.75717291        nan        nan        nan\n",
            " 0.71098211 0.7108453  0.71271843 0.7289758  0.7289758  0.7289758\n",
            " 0.72742348 0.731915   0.73941118 0.76310628 0.75581996 0.76049414\n",
            " 0.74504242 0.75364658 0.75588186        nan        nan        nan\n",
            " 0.70789541 0.71047476 0.70864906 0.7289758  0.7289758  0.7289758\n",
            " 0.73240419 0.72556948 0.73033067 0.72932182 0.73766361 0.751178\n",
            " 0.73045487 0.75261489 0.75512308        nan        nan        nan\n",
            " 0.71105701 0.71424962 0.70969142 0.7289758  0.7289758  0.7289758\n",
            " 0.72661311 0.72785938 0.72460417 0.7306096  0.73827023 0.742015\n",
            " 0.73663823 0.73095514 0.74849498        nan        nan        nan\n",
            " 0.71279026 0.71746279 0.71547175 0.7289758  0.7289758  0.7289758\n",
            " 0.72832711 0.72832711 0.72832711 0.73193626 0.73116833 0.74286399\n",
            " 0.73988449 0.73930034 0.73837602        nan        nan        nan\n",
            " 0.71274842 0.71640912 0.71602952 0.73674369 0.73518039 0.73600616\n",
            " 0.76579443 0.77502218 0.7686762  0.78627119 0.78841627 0.78685429\n",
            " 0.77975393 0.78305479 0.77715351        nan        nan        nan\n",
            " 0.70939371 0.71044994 0.71124075 0.72851466 0.72851466 0.73094052\n",
            " 0.74669491 0.74671186 0.75236348 0.76776477 0.76562702 0.75921278\n",
            " 0.76156087 0.74805349 0.7671429         nan        nan        nan\n",
            " 0.70849567 0.7108453  0.71469036 0.7289758  0.7289758  0.7289758\n",
            " 0.72800614 0.73432698 0.74214468 0.74167226 0.75712067 0.76076264\n",
            " 0.74082376 0.75124286 0.7528698         nan        nan        nan\n",
            " 0.70765537 0.71198598 0.71198598 0.7289758  0.7289758  0.7289758\n",
            " 0.73173378 0.72785938 0.72799783 0.73033969 0.7447522  0.74802893\n",
            " 0.74341213 0.75327528 0.75372457        nan        nan        nan\n",
            " 0.70918389 0.71080513 0.70868174 0.7289758  0.7289758  0.7289758\n",
            " 0.72832711 0.73114885 0.72532911 0.73323557 0.73228947 0.74382286\n",
            " 0.73866028 0.74451913 0.73229511        nan        nan        nan\n",
            " 0.70910436 0.71559652 0.71547175 0.7289758  0.7289758  0.7289758\n",
            " 0.72832711 0.72832711 0.72832711 0.72886826 0.72728766 0.73236163\n",
            " 0.74268276 0.7460208  0.74562947        nan        nan        nan\n",
            " 0.71231506 0.71365686 0.71628784 0.73867829 0.74352235 0.74124046\n",
            " 0.78953263 0.76824046 0.7677341  0.79425263 0.80247687 0.80285643\n",
            " 0.78379624 0.79108373 0.78985777        nan        nan        nan\n",
            " 0.70752058 0.70632883 0.71143161 0.72851466 0.72692181 0.72607725\n",
            " 0.7453456  0.74949414 0.74419668 0.76232456 0.76115363 0.76726584\n",
            " 0.75624771 0.76914501 0.76343194        nan        nan        nan\n",
            " 0.70716367 0.7108453  0.71344547 0.7289758  0.7289758  0.7289758\n",
            " 0.73008957 0.73793212 0.7390959  0.74978875 0.75946428 0.76900165\n",
            " 0.74503849 0.75283487 0.74466482        nan        nan        nan\n",
            " 0.70730021 0.7125856  0.71225682 0.7289758  0.7289758  0.7289758\n",
            " 0.73173378 0.72682593 0.72910987 0.73492808 0.74480528 0.75528933\n",
            " 0.739206   0.74999901 0.74844323        nan        nan        nan\n",
            " 0.70918389 0.71080513 0.70868174 0.7289758  0.7289758  0.7289758\n",
            " 0.72644115 0.73114885 0.72644115 0.72896969 0.73179323 0.74522653\n",
            " 0.74077017 0.73443724 0.74247177        nan        nan        nan\n",
            " 0.70910436 0.71559652 0.71547175 0.7289758  0.7289758  0.7289758\n",
            " 0.72832711 0.72832711 0.72832711 0.72761777 0.72796714 0.74079615\n",
            " 0.74144624 0.74323721 0.73629612        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RANDOM FOREST"
      ],
      "metadata": {
        "id": "bZjVu-6e3SCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "xtIj2iqX41Ma"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"max_features\": [1,  3,  5,  7,  9,  11],\n",
        "    \"max_samples\": [0.7, 0.8, 0.9],\n",
        "    \"max_depth\": [None, 3, 5, 7, 9, 11],\n",
        "    \"min_samples_leaf\": [1,  3,  5,  7,  9,  11],\n",
        "    'criterion': ['gini'],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators=25, random_state=random_state, n_jobs=-1, oob_score=True)\n",
        "forest_grid_search = GridSearchCV(rfc, parameters, scoring=custom_scorer, cv=skf)\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "forest_grid_search.fit(titanic_features, titanic_label)\n",
        "\n",
        "# Get the best hyperparameters and the corresponding model\n",
        "best_params = forest_grid_search.best_params_\n",
        "best_model = forest_grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Optionally, you can evaluate the model on the full data using the F-beta score with beta=0.1\n",
        "y_pred = best_model.predict(titanic_features)\n",
        "\n",
        "fbeta = fbeta_score(titanic_label, y_pred, beta=beta)\n",
        "accuracy = accuracy_score(titanic_label, y_pred)\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(metrics.classification_report(titanic_label, y_pred))\n",
        "print(metrics.confusion_matrix(titanic_label, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "qxzd9DVI4IO6",
        "outputId": "d38a5911-d0f8-46e8-eea9-90c800dd7d9e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-280ab6bfa192>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Perform grid search to find the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mforest_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitanic_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Get the best hyperparameters and the corresponding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1853\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1942\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1944\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1587\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1589\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1697\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1698\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1699\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1700\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMms33J4jeDZ"
      },
      "source": [
        "\n",
        "# 5. Write your own bagging implementation:\n",
        "  <br>5.1. Define init for our CustomBaggingClassifier\n",
        "  <br>5.2. Write fit as described in lecture: divide train data on n parts (`n_estimators` in CustomBaggingClassifier), train `base_estimator` on each part and save these models inside class\n",
        "  <br>5.3. For predictions we should use all saved models and combine their predictions (as voting)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBaggingClassifier:\n",
        "    def __init__(self, base_estimator, n_estimators, max_samples=1.0, max_features=1.0, random_state=None):\n",
        "        self.base_estimator = base_estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "        self.estimators = []\n",
        "\n",
        "    def bootstrap_sampling(self, X, y):\n",
        "        # Implement bootstrap sampling to randomly select subsets of data\n",
        "        num_samples = X.shape[0]\n",
        "        if isinstance(self.max_samples, int):\n",
        "            sample_size = self.max_samples\n",
        "        elif isinstance(self.max_samples, float):\n",
        "            sample_size = int(self.max_samples * num_samples)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid type for max_samples. It must be int or float.\")\n",
        "\n",
        "        np.random.seed(self.random_state)\n",
        "        sample_indices = np.random.choice(num_samples, size=sample_size, replace=True)\n",
        "        X_sampled = X.iloc[sample_indices]\n",
        "        y_sampled = y.iloc[sample_indices]\n",
        "        return X_sampled, y_sampled\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_features = X.shape[1]\n",
        "        if isinstance(self.max_features, int):\n",
        "            max_features = self.max_features\n",
        "        elif isinstance(self.max_features, float):\n",
        "            max_features = max(1, int(self.max_features * n_features))\n",
        "        else:\n",
        "            raise ValueError(\"Invalid type for max_features. It must be int or float.\")\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            # Get a random subset of the data using bootstrap sampling\n",
        "            X_sampled, y_sampled = self.bootstrap_sampling(X, y)\n",
        "\n",
        "            # Train a new base estimator and store it in the list\n",
        "            estimator = self.base_estimator.fit(X_sampled, y_sampled)\n",
        "            self.estimators.append(estimator)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Make predictions using majority voting for classification\n",
        "        predictions = np.zeros((X.shape[0], len(self.estimators)))\n",
        "        for i, estimator in enumerate(self.estimators):\n",
        "            predictions[:, i] = estimator.predict(X)\n",
        "\n",
        "        # Take the majority vote to make the final prediction\n",
        "        final_predictions = np.apply_along_axis(lambda x: np.bincount(x.astype('int')).argmax(), axis=1, arr=predictions)\n",
        "\n",
        "        return final_predictions\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'base_estimator': self.base_estimator,\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'max_samples': self.max_samples,\n",
        "            'max_features': self.max_features,\n",
        "            'random_state': self.random_state\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for param, value in params.items():\n",
        "            setattr(self, param, value)\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "4dFu1IPnG7T7"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    \"max_features\": [1,  3,  5,  7,  9,  11],\n",
        "    \"max_samples\": [0.7, 0.8, 0.9],\n",
        "    \"base_estimator__max_depth\": [None, 3, 5, 7, 9, 11],\n",
        "    \"base_estimator__min_samples_leaf\": [1,  3,  5,  7,  9,  11],\n",
        "    'base_estimator__criterion': ['gini'],\n",
        "    'base_estimator__splitter': ['random'],\n",
        "    'base_estimatorr__class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=random_state)\n",
        "cbg = CustomBaggingClassifier(dt, random_state=random_state, n_estimators=25)\n",
        "\n",
        "cbag_grid_search = GridSearchCV(cbg, parameters, scoring=custom_scorer, cv=skf)\n",
        "\n",
        "\n",
        "# Perform grid search to find the best hyperparameters\n",
        "cbag_grid_search.fit(titanic_features, titanic_label)\n",
        "\n",
        "# Get the best hyperparameters and the corresponding model\n",
        "best_params = cbag_grid_search.best_params_\n",
        "best_model = cbag_grid_search.best_estimator_\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Optionally, you can evaluate the model on the full data using the F-beta score with beta=0.1\n",
        "y_pred = best_model.predict(titanic_features)\n",
        "\n",
        "fbeta = fbeta_score(titanic_label, y_pred, beta=beta)\n",
        "accuracy = accuracy_score(titanic_label, y_pred)\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "print(metrics.classification_report(titanic_label, y_pred))\n",
        "print(metrics.confusion_matrix(titanic_label, y_pred))\n"
      ],
      "metadata": {
        "id": "jOXXiZT4LrlU",
        "outputId": "735486ee-4d30-4259-ae1a-588a7d60bd3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': None, 'base_estimator__min_samples_leaf': 1, 'base_estimator__splitter': 'random', 'base_estimatorr__class_weight': 'balanced', 'max_features': 1, 'max_samples': 0.7}\n",
            "F-beta Score (beta=0.5): 0.8001\n",
            "Accuracy: 0.8541\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.86      0.88       549\n",
            "           1       0.79      0.85      0.82       342\n",
            "\n",
            "    accuracy                           0.85       891\n",
            "   macro avg       0.84      0.85      0.85       891\n",
            "weighted avg       0.86      0.85      0.85       891\n",
            "\n",
            "[[472  77]\n",
            " [ 53 289]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Compare performance of sklearn bagging model with your own implementation"
      ],
      "metadata": {
        "id": "fsNWrra-_van"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CustomBaggingClassifier**\n",
        "\n",
        "Best Hyperparameters: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': None, 'base_estimator__min_samples_leaf': 1, 'base_estimator__splitter': 'random', 'base_estimatorr__class_weight': 'balanced', 'max_features': 1, 'max_samples': 0.7}\n",
        "F-beta Score (beta=0.5): 0.8001\n",
        "Accuracy: 0.8541\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.90      0.86      0.88       549\n",
        "           1       0.79      0.85      0.82       342\n",
        "\n",
        "    accuracy                           0.85       891\n",
        "   macro avg       0.84      0.85      0.85       891\n",
        "weighted avg       0.86      0.85      0.85       891\n",
        "\n",
        "[[472  77]\n",
        " [ 53 289]]"
      ],
      "metadata": {
        "id": "S35q6cG8T_5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BaggingClassifier**\n",
        "\n",
        "Best Hyperparameters: {'estimator__class_weight': 'balanced', 'estimator__criterion': 'gini', 'estimator__max_depth': None, 'estimator__min_samples_leaf': 1, 'estimator__splitter': 'random', 'max_features': 5, 'max_samples': 0.7}\n",
        "F-beta Score (beta=0.5): 0.9102\n",
        "Accuracy: 0.9147\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.91      0.96      0.93       549\n",
        "           1       0.93      0.84      0.88       342\n",
        "\n",
        "    accuracy                           0.91       891\n",
        "   macro avg       0.92      0.90      0.91       891\n",
        "weighted avg       0.92      0.91      0.91       891\n",
        "\n",
        "[[527  22]\n",
        " [ 54 288]]"
      ],
      "metadata": {
        "id": "_90_xHDiUM7V"
      }
    }
  ]
}