{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXcGNFuSIKbTsAd2sqog4q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reesha-rsh/MLb4/blob/main/Homework/HW9_Boosting_XGBoost%2C_LightGBM%2C_CatBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework:\n",
        "\n",
        "- use the dataset and validation approach which you used for previous homeworks\n",
        "- train xgboost, lightgbm, and catboost models and find the best hyperparameters for each algorithm\n",
        "- compare results between them and previously trained random forest"
      ],
      "metadata": {
        "id": "gEOgD5Ja2Ob1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init"
      ],
      "metadata": {
        "id": "OZIy0wxO5Q_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "63IgV7lh2Bks",
        "outputId": "35ae47ed-a1a8-467e-b085-5151dbfd2d31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import make_scorer, fbeta_score, classification_report, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_full = pd.read_csv(\"/content/drive/MyDrive/MLb4/EDA Titanic/train.csv\")\n",
        "test_full = pd.read_csv(\"/content/drive/MyDrive/MLb4/EDA Titanic/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 42"
      ],
      "metadata": {
        "id": "TxMq2dAL3Fij"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(df,age_median,fare_median):\n",
        "  useless_features = ['Name','Ticket','Cabin']\n",
        "  data_cleaned = df\n",
        "  data_cleaned = data_cleaned.drop(columns = useless_features)\n",
        "\n",
        "  # generate binary values using get_dummies\n",
        "  data_cleaned = pd.get_dummies(data_cleaned, columns=['Sex'],prefix=[\"Sex\"])\n",
        "  data_cleaned = pd.get_dummies(data_cleaned, columns=['Embarked'],prefix=[\"Embarked\"])\n",
        "\n",
        "  # Check for NaN values in the DataFrame\n",
        "  nan_mask = data_cleaned.isnull()\n",
        "  # Count the number of NaN values in each column\n",
        "  nan_count_per_column = data_cleaned.isnull().sum()\n",
        "\n",
        "  data_cleaned['Age'] = data_cleaned['Age'].fillna(age_median)\n",
        "  data_cleaned['Fare'] = data_cleaned['Fare'].fillna(fare_median)\n",
        "\n",
        "  return data_cleaned\n"
      ],
      "metadata": {
        "id": "ERoq9Nnv2xBB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_features = ['PassengerId', 'Survived']"
      ],
      "metadata": {
        "id": "F3H24ogz24AC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get medians that will fill NaNs in generate func\n",
        "age_median = train_full['Age'].median()\n",
        "print(age_median)\n",
        "fare_median = train_full['Fare'].median()\n",
        "print(fare_median)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwF8O6rh23jf",
        "outputId": "843be972-493c-4342-ab45-37ba9242810a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28.0\n",
            "14.4542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = generate(train_full,age_median=age_median,fare_median=fare_median)\n",
        "print(train)\n",
        "\n",
        "X_train = train.drop(not_features, axis = 1)\n",
        "y_train = train['Survived']"
      ],
      "metadata": {
        "id": "p_iH14z43CbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e02b99-6ac3-40cb-bb92-bb0afb5bc36e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare  Sex_female  \\\n",
            "0              1         0       3  22.0      1      0   7.2500           0   \n",
            "1              2         1       1  38.0      1      0  71.2833           1   \n",
            "2              3         1       3  26.0      0      0   7.9250           1   \n",
            "3              4         1       1  35.0      1      0  53.1000           1   \n",
            "4              5         0       3  35.0      0      0   8.0500           0   \n",
            "..           ...       ...     ...   ...    ...    ...      ...         ...   \n",
            "886          887         0       2  27.0      0      0  13.0000           0   \n",
            "887          888         1       1  19.0      0      0  30.0000           1   \n",
            "888          889         0       3  28.0      1      2  23.4500           1   \n",
            "889          890         1       1  26.0      0      0  30.0000           0   \n",
            "890          891         0       3  32.0      0      0   7.7500           0   \n",
            "\n",
            "     Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
            "0           1           0           0           1  \n",
            "1           0           1           0           0  \n",
            "2           0           0           0           1  \n",
            "3           0           0           0           1  \n",
            "4           1           0           0           1  \n",
            "..        ...         ...         ...         ...  \n",
            "886         1           0           0           1  \n",
            "887         0           0           0           1  \n",
            "888         0           0           0           1  \n",
            "889         1           1           0           0  \n",
            "890         1           0           1           0  \n",
            "\n",
            "[891 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = generate(test_full,age_median=age_median,fare_median=fare_median)\n",
        "X_test = test.drop('PassengerId' , axis = 1)"
      ],
      "metadata": {
        "id": "9bx8vanYmCse"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)"
      ],
      "metadata": {
        "id": "dFbDZVZw3pwl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the custom scoring function with the desired beta value\n",
        "beta = 0.5\n",
        "custom_scorer = make_scorer(fbeta_score, beta=beta)"
      ],
      "metadata": {
        "id": "QoYN1zO33roU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost"
      ],
      "metadata": {
        "id": "U-tXhUix5ULR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "k-_IOynX5czb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_train = xgb.DMatrix(X_train, y_train, feature_names=X_train.columns)\n",
        "\n",
        "num_rounds = 10000\n",
        "param = {\n",
        "    #default\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eta\": 0.01,\n",
        "    \"verbosity\": 0,\n",
        "    \"nthread\": 10,\n",
        "    \"random_seed\": random_state,\n",
        "    \"feval\": custom_scorer,\n",
        "\n",
        "\n",
        "    # regularization parameters\n",
        "    \"max_leaves\": 16,\n",
        "    \"subsample\": 0.7,\n",
        "    \"colsample_bytree\": 0.9,\n",
        "    \"gamma\": 0,\n",
        "    \"min_child_weight\": 5,\n",
        "\n",
        "\n",
        "    #lightgbm approach\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"grow_policy\": \"lossguide\"\n",
        "}\n",
        "\n",
        "results = xgb.cv(param, xgb_train, num_rounds, early_stopping_rounds=50,folds=skf, verbose_eval=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQl86_ytz0dd",
        "outputId": "143b1e00-bbe0-4da9-ec16-3f8f1bc8058e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\ttrain-logloss:0.68906+0.00013\ttest-logloss:0.68935+0.00010\n",
            "[100]\ttrain-logloss:0.46626+0.00400\ttest-logloss:0.47945+0.01062\n",
            "[200]\ttrain-logloss:0.40395+0.00450\ttest-logloss:0.43058+0.01665\n",
            "[300]\ttrain-logloss:0.37409+0.00492\ttest-logloss:0.41286+0.02047\n",
            "[400]\ttrain-logloss:0.35697+0.00516\ttest-logloss:0.40710+0.02327\n",
            "[500]\ttrain-logloss:0.34452+0.00540\ttest-logloss:0.40435+0.02584\n",
            "[600]\ttrain-logloss:0.33471+0.00581\ttest-logloss:0.40283+0.02860\n",
            "[700]\ttrain-logloss:0.32643+0.00612\ttest-logloss:0.40101+0.02967\n",
            "[800]\ttrain-logloss:0.31920+0.00632\ttest-logloss:0.39998+0.03174\n",
            "[900]\ttrain-logloss:0.31266+0.00646\ttest-logloss:0.39878+0.03275\n",
            "[1000]\ttrain-logloss:0.30672+0.00666\ttest-logloss:0.39818+0.03344\n",
            "[1100]\ttrain-logloss:0.30145+0.00673\ttest-logloss:0.39750+0.03425\n",
            "[1200]\ttrain-logloss:0.29651+0.00681\ttest-logloss:0.39664+0.03440\n",
            "[1245]\ttrain-logloss:0.29443+0.00689\ttest-logloss:0.39678+0.03457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = xgb.XGBClassifier(\n",
        "    #default\n",
        "    objective= \"binary:logistic\",\n",
        "    eta= 0.01,\n",
        "    verbosity= 0,\n",
        "    nthread= 10,\n",
        "    random_seed= random_state,\n",
        "    feval= custom_scorer,\n",
        "\n",
        "    n_estimators = 1245,\n",
        "\n",
        "\n",
        "    # regularization parameters\n",
        "    max_leaves= 16,\n",
        "    subsample= 0.7,\n",
        "    colsample_bytree= 0.9,\n",
        "    gamma= 0,\n",
        "    min_child_weight= 5,\n",
        "\n",
        "\n",
        "    #lightgbm approach\n",
        "    tree_method= \"hist\",\n",
        "    grow_policy= \"lossguide\"\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "fbeta = cross_val_score(model, X_train, y_train, scoring=custom_scorer, cv=skf).mean()\n",
        "accuracy = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=skf).mean()\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFnbW0Dq57Ju",
        "outputId": "1a585197-be0a-463c-8bb6-982f03b0021d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-beta Score (beta=0.5): 0.8168\n",
            "Accuracy: 0.8451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "submission_data = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': y_pred})\n",
        "submission_data.to_csv(\"/content/drive/MyDrive/MLb4/EDA Titanic/submissionXgb.csv\", index=False)"
      ],
      "metadata": {
        "id": "AxpiKb7tme3D"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION SCORE:** 0.76315"
      ],
      "metadata": {
        "id": "zkg8Dk5h3RLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM"
      ],
      "metadata": {
        "id": "DeI9efrdX6k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "4Cs9nADcZCRe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the F-beta score function\n",
        "def custom_fbeta_score(y_true,y_pred):\n",
        "    beta = 0.5  # Set your desired beta value\n",
        "    y_true_binary = (y_true > 0.5).astype(int)\n",
        "    return 'f_beta', fbeta_score(y_true_binary, y_pred.get_label(), beta=beta), True"
      ],
      "metadata": {
        "id": "ksejwReM-SA9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    #default\n",
        "    \"objective\": \"binary\",\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"num_threads\": 10,\n",
        "     \"metric\": \"None\",\n",
        "    \"seed\":  [random_state],\n",
        "\n",
        "    #regularization\n",
        "    \"colsample_bytree\": 0.7,\n",
        "    \"subsample\": 0.7,\n",
        "    \"subsample_freq\": 1,\n",
        "    \"min_data_in_leaf\": 7,\n",
        "    'force_col_wise': 'true'\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "n_rounds = 10000\n",
        "\n",
        "lgb_train = lgb.Dataset(X_train, label=y_train, free_raw_data=False)\n",
        "result = lgb.cv(parameters, lgb_train, n_rounds,feval=custom_fbeta_score, folds=skf, callbacks=([lgb.early_stopping(stopping_rounds=20),lgb.log_evaluation(period=10, show_stdv=True)]), eval_train_metric=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-NG30XnQj0z",
        "outputId": "8906b8b1-9b8e-4f3e-f26d-2ca4fbc25665"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 273, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 226\n",
            "[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 10\n",
            "[LightGBM] [Info] Number of positive: 274, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 226\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] Number of positive: 274, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 226\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] Number of positive: 274, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 226\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] Number of positive: 273, number of negative: 440\n",
            "[LightGBM] [Info] Total Bins 226\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383427 -> initscore=-0.475028\n",
            "[LightGBM] [Info] Start training from score -0.475028\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384292 -> initscore=-0.471371\n",
            "[LightGBM] [Info] Start training from score -0.471371\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384292 -> initscore=-0.471371\n",
            "[LightGBM] [Info] Start training from score -0.471371\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384292 -> initscore=-0.471371\n",
            "[LightGBM] [Info] Start training from score -0.471371\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382889 -> initscore=-0.477303\n",
            "[LightGBM] [Info] Start training from score -0.477303\n",
            "Training until validation scores don't improve for 20 rounds\n",
            "[10]\tcv_agg's train f_beta: 0 + 0\tcv_agg's valid f_beta: 0 + 0\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[20]\tcv_agg's train f_beta: 0 + 0\tcv_agg's valid f_beta: 0 + 0\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[30]\tcv_agg's train f_beta: 0.513944 + 0.00788529\tcv_agg's valid f_beta: 0.487517 + 0.0535153\n",
            "[40]\tcv_agg's train f_beta: 0.620386 + 0.0123121\tcv_agg's valid f_beta: 0.56629 + 0.0608858\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[50]\tcv_agg's train f_beta: 0.663727 + 0.00462079\tcv_agg's valid f_beta: 0.615264 + 0.0466076\n",
            "[60]\tcv_agg's train f_beta: 0.692299 + 0.0137899\tcv_agg's valid f_beta: 0.645313 + 0.0338959\n",
            "[70]\tcv_agg's train f_beta: 0.720709 + 0.0158139\tcv_agg's valid f_beta: 0.654433 + 0.0410043\n",
            "[80]\tcv_agg's train f_beta: 0.737888 + 0.0195027\tcv_agg's valid f_beta: 0.675367 + 0.0312882\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[90]\tcv_agg's train f_beta: 0.748804 + 0.0159307\tcv_agg's valid f_beta: 0.685979 + 0.0349937\n",
            "[100]\tcv_agg's train f_beta: 0.765362 + 0.0138701\tcv_agg's valid f_beta: 0.708588 + 0.0339561\n",
            "[110]\tcv_agg's train f_beta: 0.775227 + 0.00881819\tcv_agg's valid f_beta: 0.712991 + 0.0360907\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[120]\tcv_agg's train f_beta: 0.778712 + 0.0112603\tcv_agg's valid f_beta: 0.714826 + 0.030263\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[130]\tcv_agg's train f_beta: 0.782973 + 0.0112408\tcv_agg's valid f_beta: 0.716086 + 0.0328107\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[117]\tcv_agg's train f_beta: 0.777561 + 0.00966728\tcv_agg's valid f_beta: 0.717399 + 0.0329289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_model = lgb.LGBMClassifier(\n",
        "    objective= \"binary\",\n",
        "    learning_rate= 0.01,\n",
        "    num_threads= 10,\n",
        "    metric= \"None\",\n",
        "    seed=  random_state,\n",
        "    colsample_bytree= 0.7,\n",
        "    subsample= 0.9,\n",
        "    subsample_freq= 1,\n",
        "    min_data_in_leaf= 7,\n",
        "    force_col_wise= True,\n",
        "    n_estimators  = 125)\n",
        "\n",
        "\n",
        "lgb_model.fit(X_train, y_train,eval_metric =custom_fbeta_score)\n",
        "\n",
        "fbeta = cross_val_score(lgb_model, X_train, y_train, scoring=custom_scorer, cv=skf).mean()\n",
        "accuracy = cross_val_score(lgb_model, X_train, y_train, scoring='accuracy', cv=skf).mean()\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "submission_data = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': y_pred})\n",
        "submission_data.to_csv(\"/content/drive/MyDrive/MLb4/EDA Titanic/submissionLgb.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPLwxMS8oQFg",
        "outputId": "f57fceac-9f34-436f-b570-ec9a39f85807"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 342, number of negative: 549\n",
            "[LightGBM] [Info] Total Bins 226\n",
            "[LightGBM] [Info] Number of data points in the train set: 891, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383838 -> initscore=-0.473288\n",
            "[LightGBM] [Info] Start training from score -0.473288\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 273, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 202\n",
            "[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383427 -> initscore=-0.475028\n",
            "[LightGBM] [Info] Start training from score -0.475028\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 274, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 207\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384292 -> initscore=-0.471371\n",
            "[LightGBM] [Info] Start training from score -0.471371\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 274, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 205\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384292 -> initscore=-0.471371\n",
            "[LightGBM] [Info] Start training from score -0.471371\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 274, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 208\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384292 -> initscore=-0.471371\n",
            "[LightGBM] [Info] Start training from score -0.471371\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 273, number of negative: 440\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382889 -> initscore=-0.477303\n",
            "[LightGBM] [Info] Start training from score -0.477303\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 273, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 202\n",
            "[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383427 -> initscore=-0.475028\n",
            "[LightGBM] [Info] Start training from score -0.475028\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 274, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 207\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384292 -> initscore=-0.471371\n",
            "[LightGBM] [Info] Start training from score -0.471371\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 274, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 205\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384292 -> initscore=-0.471371\n",
            "[LightGBM] [Info] Start training from score -0.471371\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 274, number of negative: 439\n",
            "[LightGBM] [Info] Total Bins 208\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384292 -> initscore=-0.471371\n",
            "[LightGBM] [Info] Start training from score -0.471371\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "[LightGBM] [Info] Number of positive: 273, number of negative: 440\n",
            "[LightGBM] [Info] Total Bins 210\n",
            "[LightGBM] [Info] Number of data points in the train set: 713, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.382889 -> initscore=-0.477303\n",
            "[LightGBM] [Info] Start training from score -0.477303\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
            "F-beta Score (beta=0.5): 0.8170\n",
            "Accuracy: 0.8372\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION SCORE:** 0.77033"
      ],
      "metadata": {
        "id": "76ErhyjLxo1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CatBoost"
      ],
      "metadata": {
        "id": "LnXWnI1zsmMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UE4pSS2uZkz",
        "outputId": "821685de-f33a-4397-8853-b977ae03da1f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2-cp310-cp310-manylinux2014_x86_64.whl (98.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features_indices = np.where(X_train.dtypes != float)[0]"
      ],
      "metadata": {
        "id": "dFnBEy2juou9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FBetaMetric:\n",
        "\n",
        "    def __init__(self, beta):\n",
        "        self.beta = beta\n",
        "\n",
        "    @staticmethod\n",
        "    def get_fbeta(y_true, y_pred, beta):\n",
        "        y_pred_binary = (y_pred >= 0.5).astype(int)\n",
        "        return fbeta_score(y_true, y_pred_binary, beta=beta)\n",
        "\n",
        "    def is_max_optimal(self):\n",
        "        return True  # greater is better\n",
        "\n",
        "    def evaluate(self, approxes, target, weight):\n",
        "        assert len(approxes) == 1\n",
        "        assert len(target) == len(approxes[0])\n",
        "        y_true = np.array(target).astype(int)\n",
        "        approx = np.array(approxes[0])\n",
        "        score = self.get_fbeta(y_true, approx, self.beta)\n",
        "        return score, 1\n",
        "\n",
        "    def get_final_error(self, error, weight):\n",
        "        return error\n",
        "\n",
        "custom_fbeta_metric = FBetaMetric(beta=beta)\n",
        "\n"
      ],
      "metadata": {
        "id": "hPoiYg6RVlWj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import catboost as ctb\n",
        "parameters = {\n",
        "    \"loss_function\": \"Logloss\",\n",
        "    \"eval_metric\": custom_fbeta_metric,\n",
        "    \"iterations\": 10000,\n",
        "    \"learning_rate\": 0.01,\n",
        "    \"random_seed\": random_state,\n",
        "    \"od_wait\": 20,\n",
        "    \"od_type\": \"Iter\",\n",
        "    \"thread_count\": 10,\n",
        "    \"subsample\": 0.7,\n",
        "    \"colsample_bylevel\": 0.9\n",
        "}\n",
        "\n",
        "ctb_data = ctb.Pool(X_train,y_train,cat_features=categorical_features_indices)\n",
        "result = ctb.cv(ctb_data, parameters, folds=skf, seed=random_state, verbose_eval=20, plot=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvtWU03z4Rw7",
        "outputId": "ca219a58-a810-4c1b-ab32-d94b501f330a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:6655: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  return _cv(params, pool, fold_count, inverted, partition_random_seed, shuffle, stratified,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on fold [0/5]\n",
            "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 2.75s\tremaining: 7h 38m 31s\n",
            "20:\tlearn: 0.6097561\ttest: 0.6369427\tbest: 0.6369427 (20)\ttotal: 3.38s\tremaining: 26m 47s\n",
            "40:\tlearn: 0.7911001\ttest: 0.7746479\tbest: 0.7746479 (25)\ttotal: 3.92s\tremaining: 15m 51s\n",
            "\n",
            "bestTest = 0.7746478873\n",
            "bestIteration = 25\n",
            "\n",
            "Training on fold [1/5]\n",
            "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 45.7ms\tremaining: 7m 37s\n",
            "20:\tlearn: 0.6484642\ttest: 0.6250000\tbest: 0.6250000 (20)\ttotal: 693ms\tremaining: 5m 29s\n",
            "40:\tlearn: 0.7957245\ttest: 0.7500000\tbest: 0.7670455 (25)\ttotal: 1.51s\tremaining: 6m 7s\n",
            "\n",
            "bestTest = 0.7670454545\n",
            "bestIteration = 25\n",
            "\n",
            "Training on fold [2/5]\n",
            "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 35.5ms\tremaining: 5m 55s\n",
            "20:\tlearn: 0.7227139\ttest: 0.6845238\tbest: 0.6845238 (20)\ttotal: 352ms\tremaining: 2m 47s\n",
            "40:\tlearn: 0.7951807\ttest: 0.7552083\tbest: 0.7552083 (23)\ttotal: 797ms\tremaining: 3m 13s\n",
            "\n",
            "bestTest = 0.7552083333\n",
            "bestIteration = 23\n",
            "\n",
            "Training on fold [3/5]\n",
            "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 14.9ms\tremaining: 2m 28s\n",
            "20:\tlearn: 0.5898876\ttest: 0.6907895\tbest: 0.6907895 (20)\ttotal: 479ms\tremaining: 3m 47s\n",
            "40:\tlearn: 0.7706767\ttest: 0.8482143\tbest: 0.8482143 (26)\ttotal: 1.07s\tremaining: 4m 19s\n",
            "\n",
            "bestTest = 0.8482142857\n",
            "bestIteration = 26\n",
            "\n",
            "Training on fold [4/5]\n",
            "0:\tlearn: 0.0000000\ttest: 0.0000000\tbest: 0.0000000 (0)\ttotal: 23.3ms\tremaining: 3m 53s\n",
            "20:\tlearn: 0.6738769\ttest: 0.6535948\tbest: 0.6535948 (20)\ttotal: 480ms\tremaining: 3m 47s\n",
            "40:\tlearn: 0.7818411\ttest: 0.7981221\tbest: 0.7981221 (26)\ttotal: 829ms\tremaining: 3m 21s\n",
            "\n",
            "bestTest = 0.7981220657\n",
            "bestIteration = 26\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = ctb.CatBoostClassifier(\n",
        "    loss_function = \"Logloss\",\n",
        "    eval_metric = custom_fbeta_metric,\n",
        "    iterations= 26,\n",
        "    learning_rate= 0.01,\n",
        "    random_seed= random_state,\n",
        "    od_wait = 20,\n",
        "    od_type= \"Iter\",\n",
        "    thread_count= 10,\n",
        "    subsample= 0.7,\n",
        "    colsample_bylevel= 0.9)\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train,cat_features=categorical_features_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOWuBq1zQK8l",
        "outputId": "243bdf26-194d-4990-d6f3-aa652a0954a5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.0000000\ttotal: 87.5ms\tremaining: 2.19s\n",
            "1:\tlearn: 0.0000000\ttotal: 97.1ms\tremaining: 1.17s\n",
            "2:\tlearn: 0.0000000\ttotal: 105ms\tremaining: 808ms\n",
            "3:\tlearn: 0.0000000\ttotal: 113ms\tremaining: 624ms\n",
            "4:\tlearn: 0.0000000\ttotal: 122ms\tremaining: 513ms\n",
            "5:\tlearn: 0.0000000\ttotal: 129ms\tremaining: 430ms\n",
            "6:\tlearn: 0.0000000\ttotal: 136ms\tremaining: 369ms\n",
            "7:\tlearn: 0.0000000\ttotal: 144ms\tremaining: 323ms\n",
            "8:\tlearn: 0.0000000\ttotal: 152ms\tremaining: 288ms\n",
            "9:\tlearn: 0.0000000\ttotal: 161ms\tremaining: 257ms\n",
            "10:\tlearn: 0.0000000\ttotal: 171ms\tremaining: 234ms\n",
            "11:\tlearn: 0.0000000\ttotal: 180ms\tremaining: 210ms\n",
            "12:\tlearn: 0.0000000\ttotal: 186ms\tremaining: 186ms\n",
            "13:\tlearn: 0.0000000\ttotal: 190ms\tremaining: 163ms\n",
            "14:\tlearn: 0.0000000\ttotal: 193ms\tremaining: 142ms\n",
            "15:\tlearn: 0.0000000\ttotal: 198ms\tremaining: 124ms\n",
            "16:\tlearn: 0.0000000\ttotal: 201ms\tremaining: 106ms\n",
            "17:\tlearn: 0.0000000\ttotal: 205ms\tremaining: 91ms\n",
            "18:\tlearn: 0.0000000\ttotal: 211ms\tremaining: 77.9ms\n",
            "19:\tlearn: 0.0000000\ttotal: 215ms\tremaining: 64.6ms\n",
            "20:\tlearn: 0.0000000\ttotal: 219ms\tremaining: 52.2ms\n",
            "21:\tlearn: 0.0000000\ttotal: 223ms\tremaining: 40.6ms\n",
            "22:\tlearn: 0.0000000\ttotal: 226ms\tremaining: 29.5ms\n",
            "23:\tlearn: 0.0000000\ttotal: 230ms\tremaining: 19.1ms\n",
            "24:\tlearn: 0.0000000\ttotal: 233ms\tremaining: 9.31ms\n",
            "25:\tlearn: 0.0000000\ttotal: 236ms\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<catboost.core.CatBoostClassifier at 0x7dc6034856f0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fbeta = cross_val_score(clf, X_train, y_train, scoring=custom_scorer, cv=skf).mean()\n",
        "accuracy = cross_val_score(clf, X_train, y_train, scoring='accuracy', cv=skf).mean()\n",
        "\n",
        "print(\"F-beta Score (beta={}): {:.4f}\".format(beta, fbeta))\n",
        "print(\"Accuracy: {:.4f}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp03p5PKegpz",
        "outputId": "03bdeb12-d333-4617-c4f6-d0cc8896b9b4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n",
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.0000000\ttotal: 58.8ms\tremaining: 1.47s\n",
            "1:\tlearn: 0.0000000\ttotal: 62.7ms\tremaining: 752ms\n",
            "2:\tlearn: 0.0000000\ttotal: 65.6ms\tremaining: 503ms\n",
            "3:\tlearn: 0.0000000\ttotal: 68.3ms\tremaining: 376ms\n",
            "4:\tlearn: 0.0000000\ttotal: 70.9ms\tremaining: 298ms\n",
            "5:\tlearn: 0.0000000\ttotal: 73.1ms\tremaining: 244ms\n",
            "6:\tlearn: 0.0000000\ttotal: 75.6ms\tremaining: 205ms\n",
            "7:\tlearn: 0.0000000\ttotal: 77.6ms\tremaining: 175ms\n",
            "8:\tlearn: 0.0000000\ttotal: 80.2ms\tremaining: 151ms\n",
            "9:\tlearn: 0.0000000\ttotal: 82.5ms\tremaining: 132ms\n",
            "10:\tlearn: 0.0000000\ttotal: 84.8ms\tremaining: 116ms\n",
            "11:\tlearn: 0.0000000\ttotal: 87.3ms\tremaining: 102ms\n",
            "12:\tlearn: 0.0000000\ttotal: 89.6ms\tremaining: 89.6ms\n",
            "13:\tlearn: 0.0000000\ttotal: 91.7ms\tremaining: 78.6ms\n",
            "14:\tlearn: 0.0000000\ttotal: 94.1ms\tremaining: 69ms\n",
            "15:\tlearn: 0.0000000\ttotal: 96.4ms\tremaining: 60.3ms\n",
            "16:\tlearn: 0.0000000\ttotal: 98.8ms\tremaining: 52.3ms\n",
            "17:\tlearn: 0.0000000\ttotal: 101ms\tremaining: 44.9ms\n",
            "18:\tlearn: 0.0000000\ttotal: 104ms\tremaining: 38.1ms\n",
            "19:\tlearn: 0.0000000\ttotal: 106ms\tremaining: 31.8ms\n",
            "20:\tlearn: 0.0000000\ttotal: 108ms\tremaining: 25.8ms\n",
            "21:\tlearn: 0.0000000\ttotal: 110ms\tremaining: 20.1ms\n",
            "22:\tlearn: 0.0000000\ttotal: 113ms\tremaining: 14.7ms\n",
            "23:\tlearn: 0.0000000\ttotal: 115ms\tremaining: 9.61ms\n",
            "24:\tlearn: 0.0000000\ttotal: 118ms\tremaining: 4.71ms\n",
            "25:\tlearn: 0.0000000\ttotal: 120ms\tremaining: 0us\n",
            "0:\tlearn: 0.0000000\ttotal: 54.2ms\tremaining: 1.35s\n",
            "1:\tlearn: 0.0000000\ttotal: 57.9ms\tremaining: 694ms\n",
            "2:\tlearn: 0.0000000\ttotal: 60.9ms\tremaining: 467ms\n",
            "3:\tlearn: 0.0000000\ttotal: 63.8ms\tremaining: 351ms\n",
            "4:\tlearn: 0.0000000\ttotal: 66.6ms\tremaining: 280ms\n",
            "5:\tlearn: 0.0000000\ttotal: 69.2ms\tremaining: 231ms\n",
            "6:\tlearn: 0.0000000\ttotal: 71.7ms\tremaining: 195ms\n",
            "7:\tlearn: 0.0000000\ttotal: 73.8ms\tremaining: 166ms\n",
            "8:\tlearn: 0.0000000\ttotal: 76.4ms\tremaining: 144ms\n",
            "9:\tlearn: 0.0000000\ttotal: 78.8ms\tremaining: 126ms\n",
            "10:\tlearn: 0.0000000\ttotal: 81.2ms\tremaining: 111ms\n",
            "11:\tlearn: 0.0000000\ttotal: 83.8ms\tremaining: 97.7ms\n",
            "12:\tlearn: 0.0000000\ttotal: 86.4ms\tremaining: 86.4ms\n",
            "13:\tlearn: 0.0000000\ttotal: 88.8ms\tremaining: 76.1ms\n",
            "14:\tlearn: 0.0000000\ttotal: 91.3ms\tremaining: 67ms\n",
            "15:\tlearn: 0.0000000\ttotal: 93.7ms\tremaining: 58.6ms\n",
            "16:\tlearn: 0.0000000\ttotal: 96.3ms\tremaining: 51ms\n",
            "17:\tlearn: 0.0000000\ttotal: 98.2ms\tremaining: 43.6ms\n",
            "18:\tlearn: 0.0000000\ttotal: 101ms\tremaining: 37.3ms\n",
            "19:\tlearn: 0.0000000\ttotal: 107ms\tremaining: 32.1ms\n",
            "20:\tlearn: 0.0000000\ttotal: 109ms\tremaining: 26ms\n",
            "21:\tlearn: 0.0000000\ttotal: 111ms\tremaining: 20.3ms\n",
            "22:\tlearn: 0.0000000\ttotal: 114ms\tremaining: 14.8ms\n",
            "23:\tlearn: 0.0000000\ttotal: 116ms\tremaining: 9.63ms\n",
            "24:\tlearn: 0.0000000\ttotal: 118ms\tremaining: 4.71ms\n",
            "25:\tlearn: 0.0000000\ttotal: 120ms\tremaining: 0us\n",
            "0:\tlearn: 0.0000000\ttotal: 68.6ms\tremaining: 1.72s\n",
            "1:\tlearn: 0.0000000\ttotal: 71.7ms\tremaining: 860ms\n",
            "2:\tlearn: 0.0000000\ttotal: 75.8ms\tremaining: 582ms\n",
            "3:\tlearn: 0.0000000\ttotal: 82.7ms\tremaining: 455ms\n",
            "4:\tlearn: 0.0000000\ttotal: 86.9ms\tremaining: 365ms\n",
            "5:\tlearn: 0.0000000\ttotal: 90.6ms\tremaining: 302ms\n",
            "6:\tlearn: 0.0000000\ttotal: 93.6ms\tremaining: 254ms\n",
            "7:\tlearn: 0.0000000\ttotal: 96.1ms\tremaining: 216ms\n",
            "8:\tlearn: 0.0000000\ttotal: 99ms\tremaining: 187ms\n",
            "9:\tlearn: 0.0000000\ttotal: 102ms\tremaining: 163ms\n",
            "10:\tlearn: 0.0000000\ttotal: 104ms\tremaining: 142ms\n",
            "11:\tlearn: 0.0000000\ttotal: 108ms\tremaining: 126ms\n",
            "12:\tlearn: 0.0000000\ttotal: 111ms\tremaining: 111ms\n",
            "13:\tlearn: 0.0000000\ttotal: 114ms\tremaining: 97.6ms\n",
            "14:\tlearn: 0.0000000\ttotal: 116ms\tremaining: 85.2ms\n",
            "15:\tlearn: 0.0000000\ttotal: 119ms\tremaining: 74.4ms\n",
            "16:\tlearn: 0.0000000\ttotal: 122ms\tremaining: 64.5ms\n",
            "17:\tlearn: 0.0000000\ttotal: 124ms\tremaining: 55.1ms\n",
            "18:\tlearn: 0.0000000\ttotal: 126ms\tremaining: 46.5ms\n",
            "19:\tlearn: 0.0000000\ttotal: 131ms\tremaining: 39.2ms\n",
            "20:\tlearn: 0.0000000\ttotal: 135ms\tremaining: 32.1ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n",
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21:\tlearn: 0.0000000\ttotal: 138ms\tremaining: 25.1ms\n",
            "22:\tlearn: 0.0000000\ttotal: 142ms\tremaining: 18.6ms\n",
            "23:\tlearn: 0.0000000\ttotal: 146ms\tremaining: 12.1ms\n",
            "24:\tlearn: 0.0000000\ttotal: 148ms\tremaining: 5.92ms\n",
            "25:\tlearn: 0.0000000\ttotal: 151ms\tremaining: 0us\n",
            "0:\tlearn: 0.0000000\ttotal: 58.7ms\tremaining: 1.47s\n",
            "1:\tlearn: 0.0000000\ttotal: 62ms\tremaining: 744ms\n",
            "2:\tlearn: 0.0000000\ttotal: 65ms\tremaining: 499ms\n",
            "3:\tlearn: 0.0000000\ttotal: 67.8ms\tremaining: 373ms\n",
            "4:\tlearn: 0.0000000\ttotal: 70.4ms\tremaining: 296ms\n",
            "5:\tlearn: 0.0000000\ttotal: 72.9ms\tremaining: 243ms\n",
            "6:\tlearn: 0.0000000\ttotal: 75.2ms\tremaining: 204ms\n",
            "7:\tlearn: 0.0000000\ttotal: 78.6ms\tremaining: 177ms\n",
            "8:\tlearn: 0.0000000\ttotal: 83.9ms\tremaining: 159ms\n",
            "9:\tlearn: 0.0000000\ttotal: 87.8ms\tremaining: 141ms\n",
            "10:\tlearn: 0.0000000\ttotal: 92.5ms\tremaining: 126ms\n",
            "11:\tlearn: 0.0000000\ttotal: 95.1ms\tremaining: 111ms\n",
            "12:\tlearn: 0.0000000\ttotal: 97.6ms\tremaining: 97.6ms\n",
            "13:\tlearn: 0.0000000\ttotal: 99.5ms\tremaining: 85.3ms\n",
            "14:\tlearn: 0.0000000\ttotal: 102ms\tremaining: 74.7ms\n",
            "15:\tlearn: 0.0000000\ttotal: 104ms\tremaining: 65.1ms\n",
            "16:\tlearn: 0.0000000\ttotal: 106ms\tremaining: 56.3ms\n",
            "17:\tlearn: 0.0000000\ttotal: 108ms\tremaining: 48.1ms\n",
            "18:\tlearn: 0.0000000\ttotal: 110ms\tremaining: 40.7ms\n",
            "19:\tlearn: 0.0000000\ttotal: 112ms\tremaining: 33.7ms\n",
            "20:\tlearn: 0.0000000\ttotal: 114ms\tremaining: 27.2ms\n",
            "21:\tlearn: 0.0000000\ttotal: 117ms\tremaining: 21.2ms\n",
            "22:\tlearn: 0.0000000\ttotal: 119ms\tremaining: 15.5ms\n",
            "23:\tlearn: 0.0000000\ttotal: 121ms\tremaining: 10.1ms\n",
            "24:\tlearn: 0.0000000\ttotal: 126ms\tremaining: 5.04ms\n",
            "25:\tlearn: 0.0000000\ttotal: 130ms\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n",
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.0000000\ttotal: 52.7ms\tremaining: 1.32s\n",
            "1:\tlearn: 0.0000000\ttotal: 59.8ms\tremaining: 718ms\n",
            "2:\tlearn: 0.0000000\ttotal: 62.7ms\tremaining: 481ms\n",
            "3:\tlearn: 0.0000000\ttotal: 65.4ms\tremaining: 360ms\n",
            "4:\tlearn: 0.0000000\ttotal: 67.9ms\tremaining: 285ms\n",
            "5:\tlearn: 0.0000000\ttotal: 70.3ms\tremaining: 234ms\n",
            "6:\tlearn: 0.0000000\ttotal: 72.9ms\tremaining: 198ms\n",
            "7:\tlearn: 0.0000000\ttotal: 75ms\tremaining: 169ms\n",
            "8:\tlearn: 0.0000000\ttotal: 77.6ms\tremaining: 146ms\n",
            "9:\tlearn: 0.0000000\ttotal: 80.1ms\tremaining: 128ms\n",
            "10:\tlearn: 0.0000000\ttotal: 82.4ms\tremaining: 112ms\n",
            "11:\tlearn: 0.0000000\ttotal: 84.8ms\tremaining: 99ms\n",
            "12:\tlearn: 0.0000000\ttotal: 87.4ms\tremaining: 87.4ms\n",
            "13:\tlearn: 0.0000000\ttotal: 89.6ms\tremaining: 76.8ms\n",
            "14:\tlearn: 0.0000000\ttotal: 92.2ms\tremaining: 67.6ms\n",
            "15:\tlearn: 0.0000000\ttotal: 94.6ms\tremaining: 59.1ms\n",
            "16:\tlearn: 0.0000000\ttotal: 97ms\tremaining: 51.4ms\n",
            "17:\tlearn: 0.0000000\ttotal: 99.3ms\tremaining: 44.1ms\n",
            "18:\tlearn: 0.0000000\ttotal: 102ms\tremaining: 37.5ms\n",
            "19:\tlearn: 0.0000000\ttotal: 104ms\tremaining: 31.2ms\n",
            "20:\tlearn: 0.0000000\ttotal: 107ms\tremaining: 25.4ms\n",
            "21:\tlearn: 0.0000000\ttotal: 109ms\tremaining: 19.8ms\n",
            "22:\tlearn: 0.0000000\ttotal: 112ms\tremaining: 14.6ms\n",
            "23:\tlearn: 0.0000000\ttotal: 114ms\tremaining: 9.48ms\n",
            "24:\tlearn: 0.0000000\ttotal: 116ms\tremaining: 4.65ms\n",
            "25:\tlearn: 0.0000000\ttotal: 119ms\tremaining: 0us\n",
            "0:\tlearn: 0.0000000\ttotal: 57.9ms\tremaining: 1.45s\n",
            "1:\tlearn: 0.0000000\ttotal: 60.9ms\tremaining: 730ms\n",
            "2:\tlearn: 0.0000000\ttotal: 63.6ms\tremaining: 488ms\n",
            "3:\tlearn: 0.0000000\ttotal: 66.3ms\tremaining: 365ms\n",
            "4:\tlearn: 0.0000000\ttotal: 68.9ms\tremaining: 290ms\n",
            "5:\tlearn: 0.0000000\ttotal: 71.3ms\tremaining: 238ms\n",
            "6:\tlearn: 0.0000000\ttotal: 74.1ms\tremaining: 201ms\n",
            "7:\tlearn: 0.0000000\ttotal: 76.3ms\tremaining: 172ms\n",
            "8:\tlearn: 0.0000000\ttotal: 79.5ms\tremaining: 150ms\n",
            "9:\tlearn: 0.0000000\ttotal: 81.9ms\tremaining: 131ms\n",
            "10:\tlearn: 0.0000000\ttotal: 84.3ms\tremaining: 115ms\n",
            "11:\tlearn: 0.0000000\ttotal: 86.8ms\tremaining: 101ms\n",
            "12:\tlearn: 0.0000000\ttotal: 89.3ms\tremaining: 89.3ms\n",
            "13:\tlearn: 0.0000000\ttotal: 91.6ms\tremaining: 78.5ms\n",
            "14:\tlearn: 0.0000000\ttotal: 94.2ms\tremaining: 69.1ms\n",
            "15:\tlearn: 0.0000000\ttotal: 97.8ms\tremaining: 61.1ms\n",
            "16:\tlearn: 0.0000000\ttotal: 101ms\tremaining: 53.6ms\n",
            "17:\tlearn: 0.0000000\ttotal: 105ms\tremaining: 46.8ms\n",
            "18:\tlearn: 0.0000000\ttotal: 108ms\tremaining: 39.8ms\n",
            "19:\tlearn: 0.0000000\ttotal: 111ms\tremaining: 33.2ms\n",
            "20:\tlearn: 0.0000000\ttotal: 113ms\tremaining: 26.9ms\n",
            "21:\tlearn: 0.0000000\ttotal: 115ms\tremaining: 21ms\n",
            "22:\tlearn: 0.0000000\ttotal: 118ms\tremaining: 15.4ms\n",
            "23:\tlearn: 0.0000000\ttotal: 120ms\tremaining: 10ms\n",
            "24:\tlearn: 0.0000000\ttotal: 123ms\tremaining: 4.92ms\n",
            "25:\tlearn: 0.0000000\ttotal: 125ms\tremaining: 0us\n",
            "0:\tlearn: 0.0000000\ttotal: 49ms\tremaining: 1.22s\n",
            "1:\tlearn: 0.0000000\ttotal: 51.9ms\tremaining: 623ms\n",
            "2:\tlearn: 0.0000000\ttotal: 54.7ms\tremaining: 419ms\n",
            "3:\tlearn: 0.0000000\ttotal: 57.4ms\tremaining: 316ms\n",
            "4:\tlearn: 0.0000000\ttotal: 60ms\tremaining: 252ms\n",
            "5:\tlearn: 0.0000000\ttotal: 62.4ms\tremaining: 208ms\n",
            "6:\tlearn: 0.0000000\ttotal: 64.8ms\tremaining: 176ms\n",
            "7:\tlearn: 0.0000000\ttotal: 66.9ms\tremaining: 151ms\n",
            "8:\tlearn: 0.0000000\ttotal: 69.5ms\tremaining: 131ms\n",
            "9:\tlearn: 0.0000000\ttotal: 72ms\tremaining: 115ms\n",
            "10:\tlearn: 0.0000000\ttotal: 74.3ms\tremaining: 101ms\n",
            "11:\tlearn: 0.0000000\ttotal: 76.8ms\tremaining: 89.6ms\n",
            "12:\tlearn: 0.0000000\ttotal: 79.3ms\tremaining: 79.3ms\n",
            "13:\tlearn: 0.0000000\ttotal: 81.6ms\tremaining: 70ms\n",
            "14:\tlearn: 0.0000000\ttotal: 84.4ms\tremaining: 61.9ms\n",
            "15:\tlearn: 0.0000000\ttotal: 87ms\tremaining: 54.4ms\n",
            "16:\tlearn: 0.0000000\ttotal: 89.6ms\tremaining: 47.4ms\n",
            "17:\tlearn: 0.0000000\ttotal: 91.8ms\tremaining: 40.8ms\n",
            "18:\tlearn: 0.0000000\ttotal: 94.3ms\tremaining: 34.7ms\n",
            "19:\tlearn: 0.0000000\ttotal: 96.5ms\tremaining: 28.9ms\n",
            "20:\tlearn: 0.0000000\ttotal: 98.8ms\tremaining: 23.5ms\n",
            "21:\tlearn: 0.0000000\ttotal: 101ms\tremaining: 18.4ms\n",
            "22:\tlearn: 0.0000000\ttotal: 104ms\tremaining: 13.5ms\n",
            "23:\tlearn: 0.0000000\ttotal: 106ms\tremaining: 8.85ms\n",
            "24:\tlearn: 0.0000000\ttotal: 109ms\tremaining: 4.35ms\n",
            "25:\tlearn: 0.0000000\ttotal: 111ms\tremaining: 0us\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n",
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.0000000\ttotal: 55ms\tremaining: 1.37s\n",
            "1:\tlearn: 0.0000000\ttotal: 59ms\tremaining: 708ms\n",
            "2:\tlearn: 0.0000000\ttotal: 61.4ms\tremaining: 471ms\n",
            "3:\tlearn: 0.0000000\ttotal: 63.7ms\tremaining: 350ms\n",
            "4:\tlearn: 0.0000000\ttotal: 65.9ms\tremaining: 277ms\n",
            "5:\tlearn: 0.0000000\ttotal: 67.9ms\tremaining: 226ms\n",
            "6:\tlearn: 0.0000000\ttotal: 69.9ms\tremaining: 190ms\n",
            "7:\tlearn: 0.0000000\ttotal: 71.7ms\tremaining: 161ms\n",
            "8:\tlearn: 0.0000000\ttotal: 74ms\tremaining: 140ms\n",
            "9:\tlearn: 0.0000000\ttotal: 76.4ms\tremaining: 122ms\n",
            "10:\tlearn: 0.0000000\ttotal: 78.7ms\tremaining: 107ms\n",
            "11:\tlearn: 0.0000000\ttotal: 81.2ms\tremaining: 94.7ms\n",
            "12:\tlearn: 0.0000000\ttotal: 83.6ms\tremaining: 83.6ms\n",
            "13:\tlearn: 0.0000000\ttotal: 86ms\tremaining: 73.7ms\n",
            "14:\tlearn: 0.0000000\ttotal: 88.1ms\tremaining: 64.6ms\n",
            "15:\tlearn: 0.0000000\ttotal: 94.6ms\tremaining: 59.1ms\n",
            "16:\tlearn: 0.0000000\ttotal: 97.8ms\tremaining: 51.8ms\n",
            "17:\tlearn: 0.0000000\ttotal: 99.8ms\tremaining: 44.4ms\n",
            "18:\tlearn: 0.0000000\ttotal: 102ms\tremaining: 37.6ms\n",
            "19:\tlearn: 0.0000000\ttotal: 104ms\tremaining: 31.3ms\n",
            "20:\tlearn: 0.0000000\ttotal: 106ms\tremaining: 25.3ms\n",
            "21:\tlearn: 0.0000000\ttotal: 108ms\tremaining: 19.7ms\n",
            "22:\tlearn: 0.0000000\ttotal: 111ms\tremaining: 14.4ms\n",
            "23:\tlearn: 0.0000000\ttotal: 113ms\tremaining: 9.39ms\n",
            "24:\tlearn: 0.0000000\ttotal: 115ms\tremaining: 4.59ms\n",
            "25:\tlearn: 0.0000000\ttotal: 117ms\tremaining: 0us\n",
            "0:\tlearn: 0.0000000\ttotal: 49.5ms\tremaining: 1.24s\n",
            "1:\tlearn: 0.0000000\ttotal: 52ms\tremaining: 624ms\n",
            "2:\tlearn: 0.0000000\ttotal: 54.4ms\tremaining: 417ms\n",
            "3:\tlearn: 0.0000000\ttotal: 56.8ms\tremaining: 313ms\n",
            "4:\tlearn: 0.0000000\ttotal: 59ms\tremaining: 248ms\n",
            "5:\tlearn: 0.0000000\ttotal: 61.2ms\tremaining: 204ms\n",
            "6:\tlearn: 0.0000000\ttotal: 63.3ms\tremaining: 172ms\n",
            "7:\tlearn: 0.0000000\ttotal: 65.5ms\tremaining: 147ms\n",
            "8:\tlearn: 0.0000000\ttotal: 68ms\tremaining: 128ms\n",
            "9:\tlearn: 0.0000000\ttotal: 70.4ms\tremaining: 113ms\n",
            "10:\tlearn: 0.0000000\ttotal: 72.7ms\tremaining: 99.2ms\n",
            "11:\tlearn: 0.0000000\ttotal: 75.2ms\tremaining: 87.7ms\n",
            "12:\tlearn: 0.0000000\ttotal: 77.6ms\tremaining: 77.6ms\n",
            "13:\tlearn: 0.0000000\ttotal: 79.7ms\tremaining: 68.3ms\n",
            "14:\tlearn: 0.0000000\ttotal: 82.2ms\tremaining: 60.3ms\n",
            "15:\tlearn: 0.0000000\ttotal: 84.7ms\tremaining: 52.9ms\n",
            "16:\tlearn: 0.0000000\ttotal: 87.1ms\tremaining: 46.1ms\n",
            "17:\tlearn: 0.0000000\ttotal: 89.1ms\tremaining: 39.6ms\n",
            "18:\tlearn: 0.0000000\ttotal: 91.5ms\tremaining: 33.7ms\n",
            "19:\tlearn: 0.0000000\ttotal: 93.6ms\tremaining: 28.1ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n",
            "/usr/local/lib/python3.10/dist-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
            "  _check_train_params(params)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20:\tlearn: 0.0000000\ttotal: 98.5ms\tremaining: 23.4ms\n",
            "21:\tlearn: 0.0000000\ttotal: 102ms\tremaining: 18.6ms\n",
            "22:\tlearn: 0.0000000\ttotal: 106ms\tremaining: 13.8ms\n",
            "23:\tlearn: 0.0000000\ttotal: 108ms\tremaining: 8.99ms\n",
            "24:\tlearn: 0.0000000\ttotal: 110ms\tremaining: 4.39ms\n",
            "25:\tlearn: 0.0000000\ttotal: 112ms\tremaining: 0us\n",
            "0:\tlearn: 0.0000000\ttotal: 50.3ms\tremaining: 1.26s\n",
            "1:\tlearn: 0.0000000\ttotal: 55.3ms\tremaining: 664ms\n",
            "2:\tlearn: 0.0000000\ttotal: 59.5ms\tremaining: 456ms\n",
            "3:\tlearn: 0.0000000\ttotal: 64.7ms\tremaining: 356ms\n",
            "4:\tlearn: 0.0000000\ttotal: 68.7ms\tremaining: 288ms\n",
            "5:\tlearn: 0.0000000\ttotal: 72.8ms\tremaining: 243ms\n",
            "6:\tlearn: 0.0000000\ttotal: 77.2ms\tremaining: 210ms\n",
            "7:\tlearn: 0.0000000\ttotal: 81.6ms\tremaining: 184ms\n",
            "8:\tlearn: 0.0000000\ttotal: 84.2ms\tremaining: 159ms\n",
            "9:\tlearn: 0.0000000\ttotal: 87.1ms\tremaining: 139ms\n",
            "10:\tlearn: 0.0000000\ttotal: 89.9ms\tremaining: 123ms\n",
            "11:\tlearn: 0.0000000\ttotal: 93ms\tremaining: 108ms\n",
            "12:\tlearn: 0.0000000\ttotal: 95.7ms\tremaining: 95.7ms\n",
            "13:\tlearn: 0.0000000\ttotal: 98.3ms\tremaining: 84.2ms\n",
            "14:\tlearn: 0.0000000\ttotal: 101ms\tremaining: 74.2ms\n",
            "15:\tlearn: 0.0000000\ttotal: 104ms\tremaining: 64.8ms\n",
            "16:\tlearn: 0.0000000\ttotal: 106ms\tremaining: 56.3ms\n",
            "17:\tlearn: 0.0000000\ttotal: 109ms\tremaining: 48.3ms\n",
            "18:\tlearn: 0.0000000\ttotal: 111ms\tremaining: 41.1ms\n",
            "19:\tlearn: 0.0000000\ttotal: 114ms\tremaining: 34.3ms\n",
            "20:\tlearn: 0.0000000\ttotal: 117ms\tremaining: 27.9ms\n",
            "21:\tlearn: 0.0000000\ttotal: 120ms\tremaining: 21.8ms\n",
            "22:\tlearn: 0.0000000\ttotal: 122ms\tremaining: 16ms\n",
            "23:\tlearn: 0.0000000\ttotal: 125ms\tremaining: 10.4ms\n",
            "24:\tlearn: 0.0000000\ttotal: 128ms\tremaining: 5.1ms\n",
            "25:\tlearn: 0.0000000\ttotal: 130ms\tremaining: 0us\n",
            "F-beta Score (beta=0.5): 0.7691\n",
            "Accuracy: 0.8070\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "submission_data = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': y_pred})\n",
        "submission_data.to_csv(\"/content/drive/MyDrive/MLb4/EDA Titanic/submissionCat.csv\", index=False)"
      ],
      "metadata": {
        "id": "rw8v-1We7o7V"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION SCORE:** 0.76555"
      ],
      "metadata": {
        "id": "umzw7eum8YVv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "\n",
        "\n",
        "- **Random forest**\n",
        "  - Submission Accuracy: 0.76555\n",
        "  - F-beta Score (beta=0.5): 0.8119\n",
        "  - Accuracy: 0.8440\n",
        "- **XGBoost**\n",
        "  - Submission Accuracy: 0.76315\n",
        "  - F-beta Score (beta=0.5): 0.8168\n",
        "  - Accuracy: 0.8451\n",
        "- **LightGBM**\n",
        "  - Submission Accuracy: 0.77033\n",
        "  - F-beta Score (beta=0.5): 0.8170\n",
        "  - Accuracy: 0.8372\n",
        "- **CatBoost**:\n",
        "  - Submission Accuracy: 0.76555\n",
        "  - F-beta Score (beta=0.5): 0.7691\n",
        "  - Accuracy: 0.8070\n",
        "\n"
      ],
      "metadata": {
        "id": "d1E7tQ2C8z0O"
      }
    }
  ]
}